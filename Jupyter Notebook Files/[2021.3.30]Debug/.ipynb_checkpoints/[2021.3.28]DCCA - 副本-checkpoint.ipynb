{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import numpy\n",
    "import random\n",
    "torch.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 生成两个数据集，各有200个数据，X^1是维度为2，X^2维度为10，其中每个数据中前100个标签为0，后100个标签为1。\n",
    "X1, y1 = make_moons(200, noise=0.2, shuffle=False, random_state=7)\n",
    "X2, y2 = make_blobs(n_samples=200, n_features=10, shuffle=False, centers=2, center_box=(-10, 10), random_state=7)\n",
    "\n",
    "# 对两个数据集进行降维，用KMeans，聚为两类\n",
    "km1 = KMeans(n_clusters=2, random_state=0)\n",
    "km2 = KMeans(n_clusters=2, random_state=0)\n",
    "a1 = km1.fit(X1)\n",
    "a2 = km2.fit(X2)\n",
    "\n",
    "# 这是每个聚类的中心\n",
    "X1_center_1 = km1.cluster_centers_[0, :]\n",
    "X1_center_2 = km1.cluster_centers_[1, :]\n",
    "X2_center_1 = km2.cluster_centers_[0, :]\n",
    "X2_center_2 = km2.cluster_centers_[1, :]\n",
    "\n",
    "# 用聚类后每个点到聚类中心的欧式距离来算出属于某聚类的概率，从而生成A矩阵\n",
    "def create_A_matrix(X, center1, center2):\n",
    "    A = np.zeros((200, 2))\n",
    "\n",
    "    for i in range(200):\n",
    "        dis1 = np.sqrt(np.sum(np.square(X[i, :] - center1)))\n",
    "        dis2 = np.sqrt(np.sum(np.square(X[i, :] - center2)))\n",
    "        A[i, 0] = dis1 / (dis1 + dis2)\n",
    "        A[i, 1] = dis2 / (dis1 + dis2)\n",
    "\n",
    "    return A\n",
    "\n",
    "# 生成A矩阵\n",
    "A1 = np.mat(create_A_matrix(X1, X1_center_1, X1_center_2))\n",
    "A2 = np.mat(create_A_matrix(X2, X2_center_1, X2_center_2))\n",
    "\n",
    "#将矩阵变为tensor\n",
    "X1 = torch.from_numpy(X1).double()\n",
    "X2 = torch.from_numpy(X2).double()\n",
    "A1 = torch.from_numpy(A1).double()\n",
    "A2 = torch.from_numpy(A2).double()\n",
    "\n",
    "y = y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#搭建感知机网络\n",
    "class MlpNet(nn.Module):\n",
    "    def __init__(self, layer_sizes, input_size):\n",
    "        super(MlpNet, self).__init__()\n",
    "        layers = []\n",
    "        layer_sizes = [input_size] + layer_sizes\n",
    "        for l_id in range(len(layer_sizes) - 1):\n",
    "            if l_id == len(layer_sizes) - 2:\n",
    "                layers.append(nn.Sequential(\n",
    "                    nn.BatchNorm1d(num_features=layer_sizes[l_id], affine=False, track_running_stats=True),\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
    "                ))\n",
    "            else:\n",
    "                layers.append(nn.Sequential(\n",
    "                    nn.Linear(layer_sizes[l_id], layer_sizes[l_id + 1]),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.BatchNorm1d(num_features=layer_sizes[l_id + 1], affine=False, track_running_stats=True),\n",
    "                ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#搭建深度模型\n",
    "class DeepCCA(nn.Module):\n",
    "    def __init__(self, layer_sizes1, layer_sizes2, input_size1, input_size2, outdim_size, use_all_singular_values, device=torch.device('cpu')):\n",
    "        super(DeepCCA, self).__init__()\n",
    "        self.model1 = MlpNet(layer_sizes1, input_size1).double()\n",
    "        self.model2 = MlpNet(layer_sizes2, input_size2).double()\n",
    "\n",
    "        self.loss = cca_loss(outdim_size, use_all_singular_values, device).loss\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        # feature * batch_size\n",
    "        output1 = self.model1(x1)\n",
    "        output2 = self.model2(x2)\n",
    "\n",
    "        return output1, output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Loss(Corr)\n",
    "class cca_loss():\n",
    "    def __init__(self, outdim_size, use_all_singular_values, device):\n",
    "        self.outdim_size = outdim_size\n",
    "        self.use_all_singular_values = use_all_singular_values\n",
    "        self.device = device\n",
    "        print(device)\n",
    "\n",
    "    def loss(self, H1, H2):\n",
    "\n",
    "        r1 = 1e-3\n",
    "        r2 = 1e-3\n",
    "        eps = 1e-9\n",
    "\n",
    "        H1, H2 = H1.t(), H2.t()\n",
    "        o1 = o2 = H1.size(0)\n",
    "\n",
    "        m = H1.size(1)\n",
    "\n",
    "        H1bar = H1 - H1.mean(dim=1).unsqueeze(dim=1)\n",
    "        H2bar = H2 - H2.mean(dim=1).unsqueeze(dim=1)\n",
    "\n",
    "\n",
    "        SigmaHat12 = (1.0 / (m - 1)) * torch.mm(H1bar, H2bar.t())\n",
    "        SigmaHat11 = (1.0 / (m - 1)) * torch.mm(H1bar, H1bar.t()) + r1 * torch.eye(o1, device=self.device)\n",
    "        SigmaHat22 = (1.0 / (m - 1)) * torch.mm(H2bar, H2bar.t()) + r2 * torch.eye(o2, device=self.device)\n",
    "     \n",
    "        [D1, V1] = torch.symeig(SigmaHat11, eigenvectors=True)\n",
    "        [D2, V2] = torch.symeig(SigmaHat22, eigenvectors=True)\n",
    "     \n",
    "        # Added to increase stability\n",
    "        posInd1 = torch.gt(D1, eps).nonzero()[:, 0]\n",
    "        D1 = D1[posInd1]\n",
    "        V1 = V1[:, posInd1]\n",
    "        posInd2 = torch.gt(D2, eps).nonzero()[:, 0]\n",
    "        D2 = D2[posInd2]\n",
    "        V2 = V2[:, posInd2]\n",
    "\n",
    "        SigmaHat11RootInv = torch.mm(torch.mm(V1, torch.diag(D1 ** -0.5)), V1.t())\n",
    "        SigmaHat22RootInv = torch.mm(torch.mm(V2, torch.diag(D2 ** -0.5)), V2.t())\n",
    "\n",
    "        Tval = torch.mm(torch.mm(SigmaHat11RootInv,\n",
    "                                         SigmaHat12), SigmaHat22RootInv)\n",
    "\n",
    "\n",
    "        if self.use_all_singular_values:\n",
    "            # all singular values are used to calculate the correlation\n",
    "            tmp = torch.mm(Tval.t(), Tval)\n",
    "            corr = torch.trace(torch.sqrt(tmp))\n",
    "            # assert torch.isnan(corr).item() == 0\n",
    "        else:\n",
    "            # just the top self.outdim_size singular values are used\n",
    "            trace_TT = torch.mm(Tval.t(), Tval)\n",
    "            trace_TT = torch.add(trace_TT, (torch.eye(trace_TT.shape[0])*r1).to(self.device)) # regularization for more stability\n",
    "            U, V = torch.symeig(trace_TT, eigenvectors=True)\n",
    "            U = torch.where(U>eps, U, (torch.ones(U.shape).double()*eps).to(self.device))\n",
    "            U = U.topk(self.outdim_size)[0]\n",
    "            corr = torch.sum(torch.sqrt(U))\n",
    "\n",
    "\n",
    "\n",
    "        return -corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Solver():\n",
    "    def __init__(self, model, outdim_size, epoch_num, learning_rate, reg_par,\n",
    "                 device=torch.device('cpu')):\n",
    "        self.model = nn.DataParallel(model)\n",
    "        self.model.to(device)\n",
    "        self.epoch_num = epoch_num\n",
    "        self.loss = model.loss\n",
    "        self.optimizer = torch.optim.RMSprop(\n",
    "            self.model.parameters(), lr=learning_rate, weight_decay=reg_par)\n",
    "        self.device = device\n",
    "        self.outdim_size = outdim_size\n",
    "\n",
    "\n",
    "    def fit(self, x1, x2, tst1 = None, tst2 = None):\n",
    "        \n",
    "        x1.to(self.device)\n",
    "        x2.to(self.device)\n",
    "        \n",
    "        if tst1 is not None and tst2 is not None:\n",
    "            tst1.to(self.device)\n",
    "            tst2.to(self.device)\n",
    "\n",
    "        train_losses = []\n",
    "        for epoch in range(self.epoch_num):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            batch_x1 = x1\n",
    "            batch_x2 = x2\n",
    "            o1, o2 = self.model(batch_x1, batch_x2)\n",
    "            loss = self.loss(o1, o2)\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss = np.mean(train_losses)\n",
    "            print('Epoch:', epoch + 1, '    Loss:', train_loss)\n",
    "            if epoch % 100 == 0:\n",
    "                o1_ = o1.cpu().detach().numpy()\n",
    "                o2_ = o2.cpu().detach().numpy()\n",
    "                #visualization_LR(o1_, y, 'View 1')\n",
    "                #visualization_LR(o2_, y, 'View 2')\n",
    "\n",
    "\n",
    "    def test(self, x1, x2, use_linear_cca=False):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.model.eval()\n",
    "            losses = []\n",
    "            outputs1 = []\n",
    "            outputs2 = []\n",
    "            o1, o2 = self.model(x1, x2)\n",
    "            outputs1.append(o1)\n",
    "            outputs2.append(o2)\n",
    "            loss = self.loss(o1,o2)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "        outputs = [torch.cat(outputs1, dim=0).cpu().numpy(),\n",
    "                   torch.cat(outputs2, dim=0).cpu().numpy()]\n",
    "        return losses, outputs\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "#精确度分析\n",
    "def acc(y_pred, y_true):\n",
    "    return accuracy_score(y_true, y_pred, normalize=True)\n",
    "\n",
    "# 线性分类器\n",
    "def LR(X_train, y_train, y_test=None):\n",
    "    clf = LogisticRegression(penalty='l2')\n",
    "    clf.fit(X_train, y_train)  \n",
    "    return clf.coef_, clf.intercept_, clf.predict(X_train)\n",
    "\n",
    "#预留20%为测试集，在测试集上的精度\n",
    "def LR_2(Train_x, Train_y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(Train_x, Train_y, test_size=0.2, random_state=2)\n",
    "    clf1 = LogisticRegression(penalty='l2')\n",
    "    clf1.fit(train_x, train_y)\n",
    "    pred_y = clf1.predict(test_x)\n",
    "    acc = accuracy_score(test_y, pred_y, normalize=True)\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, pred_y, pos_label=1)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    return acc, auc_\n",
    "\n",
    "#非线性分类器\n",
    "def svm(X_train, y_train):\n",
    "    nlc = SVC(kernel='rbf')\n",
    "    nlc.fit(X_train, y_train)\n",
    "    return nlc\n",
    "\n",
    "def svm_2(Train_x, Train_y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(Train_x, Train_y, test_size=0.2, random_state=2)\n",
    "    nlc1 = SVC(kernel='rbf')\n",
    "    nlc1.fit(train_x, train_y)\n",
    "    pred_y = nlc1.predict(test_x)\n",
    "    acc = accuracy_score(test_y, pred_y, normalize=True)\n",
    "    fpr, tpr, thresholds = roc_curve(test_y, pred_y, pos_label=1)\n",
    "    auc_ = auc(fpr, tpr)\n",
    "    return acc, auc_\n",
    "\n",
    "\n",
    "# 可视化\n",
    "def visualization_LR(X, y, title):\n",
    "    weight = LR(X, y)[0]\n",
    "    bias = LR(X, y)[1]\n",
    "    dataArr = np.array(X)\n",
    "    xcord1 = [];\n",
    "    ycord1 = []\n",
    "    xcord2 = [];\n",
    "    ycord2 = []\n",
    "    for i in range(y.shape[0]):\n",
    "        if int(y[i]) == 1:\n",
    "            xcord1.append(dataArr[i, 0]);\n",
    "            ycord1.append(dataArr[i, 1])\n",
    "        else:\n",
    "            xcord2.append(dataArr[i, 0]);\n",
    "            ycord2.append(dataArr[i, 1])\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.scatter(xcord1, ycord1, s=30, c='red', marker='s')\n",
    "    ax.scatter(xcord2, ycord2, s=30, c='green')\n",
    "    x_1 = np.arange(-1, 2, 0.01)\n",
    "    a = -weight[0, 0] / weight[0, 1]\n",
    "    b = -bias / weight[0, 1]\n",
    "    #y_1 = a * x_1 + b\n",
    "    # -weights[0]\n",
    "    #ax.plot(x_1, y_1)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "def visualization_SVM(X, y, title):\n",
    "    xx, yy = np.meshgrid(np.linspace(-3, 3, 500), np.linspace(-3, 3, 500))\n",
    "    xy = np.vstack([xx.ravel(), yy.ravel()]).T\n",
    "    Z = svm(X, y).decision_function(xy).reshape(xx.shape)\n",
    "    #plt.imshow(Z, interpolation='nearest', extent=(xx.min(), xx.max(), yy.min(), yy.max(),), aspect='auto',\n",
    "               #origin='lower', cmap=plt.cm.PuOr_r)\n",
    "    plt.contour(xx, yy, Z, levels=[0], linewidths=2, linestyle='-')\n",
    "    plt.scatter(X[:, 0].tolist(), X[:, 1].tolist(), s=30, c=y, cmap=plt.cm.Paired, edgecolors='k')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Epoch: 1     Loss: -0.7201978224770378\n",
      "Epoch: 2     Loss: -0.8427655168877166\n",
      "Epoch: 3     Loss: -0.9011358026001632\n",
      "Epoch: 4     Loss: -0.9390256105383943\n",
      "Epoch: 5     Loss: -0.9666786741527458\n",
      "Epoch: 6     Loss: -0.9876749729156374\n",
      "Epoch: 7     Loss: -1.0056883693984793\n",
      "Epoch: 8     Loss: -1.0071254891126609\n",
      "Epoch: 9     Loss: -1.015432542269809\n",
      "Epoch: 10     Loss: -1.0240353601095709\n",
      "Epoch: 11     Loss: -1.0318780137445305\n",
      "Epoch: 12     Loss: -1.039079973306519\n",
      "Epoch: 13     Loss: -1.0455972889964686\n",
      "Epoch: 14     Loss: -1.0520062084173456\n",
      "Epoch: 15     Loss: -1.0582309154839573\n",
      "Epoch: 16     Loss: -1.0632401405562948\n",
      "Epoch: 17     Loss: -1.06864122324968\n",
      "Epoch: 18     Loss: -1.0726471156388435\n",
      "Epoch: 19     Loss: -1.0789694300214314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\Pytorch\\lib\\site-packages\\ipykernel_launcher.py:32: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero()\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(*, bool as_tuple) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:766.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20     Loss: -1.0852248546600392\n",
      "Epoch: 21     Loss: -1.0902637424003156\n",
      "Epoch: 22     Loss: -1.0952809247346027\n",
      "Epoch: 23     Loss: -1.099725519734995\n",
      "Epoch: 24     Loss: -1.1053892660664015\n",
      "Epoch: 25     Loss: -1.110259823513311\n",
      "Epoch: 26     Loss: -1.1150699055644275\n",
      "Epoch: 27     Loss: -1.1174215119226905\n",
      "Epoch: 28     Loss: -1.1214098652633182\n",
      "Epoch: 29     Loss: -1.1258932862636228\n",
      "Epoch: 30     Loss: -1.1307501569753704\n",
      "Epoch: 31     Loss: -1.1343661738993598\n",
      "Epoch: 32     Loss: -1.138497516003491\n",
      "Epoch: 33     Loss: -1.141028565525673\n",
      "Epoch: 34     Loss: -1.1431765108983005\n",
      "Epoch: 35     Loss: -1.144762273476787\n",
      "Epoch: 36     Loss: -1.1472391641053197\n",
      "Epoch: 37     Loss: -1.1496390044862044\n",
      "Epoch: 38     Loss: -1.1517548516357146\n",
      "Epoch: 39     Loss: -1.1530396830588159\n",
      "Epoch: 40     Loss: -1.155615603741558\n",
      "Epoch: 41     Loss: -1.158420197467783\n",
      "Epoch: 42     Loss: -1.1612107349739886\n",
      "Epoch: 43     Loss: -1.1639094047974081\n",
      "Epoch: 44     Loss: -1.165978264136042\n",
      "Epoch: 45     Loss: -1.1691545362875084\n",
      "Epoch: 46     Loss: -1.1724085204424342\n",
      "Epoch: 47     Loss: -1.17545168526821\n",
      "Epoch: 48     Loss: -1.1781173683276824\n",
      "Epoch: 49     Loss: -1.180142798805443\n",
      "Epoch: 50     Loss: -1.1832829196317445\n",
      "Epoch: 51     Loss: -1.1864258324274572\n",
      "Epoch: 52     Loss: -1.1891856543855115\n",
      "Epoch: 53     Loss: -1.191865079606297\n",
      "Epoch: 54     Loss: -1.193839673579247\n",
      "Epoch: 55     Loss: -1.1960896661377056\n",
      "Epoch: 56     Loss: -1.1989584370507118\n",
      "Epoch: 57     Loss: -1.2016002410241433\n",
      "Epoch: 58     Loss: -1.2043025315795528\n",
      "Epoch: 59     Loss: -1.2070217304285784\n",
      "Epoch: 60     Loss: -1.2092405842561373\n",
      "Epoch: 61     Loss: -1.2110710156505464\n",
      "Epoch: 62     Loss: -1.21314709540688\n",
      "Epoch: 63     Loss: -1.2157363675268367\n",
      "Epoch: 64     Loss: -1.2178225352713925\n",
      "Epoch: 65     Loss: -1.2206379005816552\n",
      "Epoch: 66     Loss: -1.2232713194471745\n",
      "Epoch: 67     Loss: -1.22569770793356\n",
      "Epoch: 68     Loss: -1.2250693587968156\n",
      "Epoch: 69     Loss: -1.2271397368959343\n",
      "Epoch: 70     Loss: -1.2295987502450054\n",
      "Epoch: 71     Loss: -1.2319211659428935\n",
      "Epoch: 72     Loss: -1.2342795433044227\n",
      "Epoch: 73     Loss: -1.236254692692258\n",
      "Epoch: 74     Loss: -1.2386295401661533\n",
      "Epoch: 75     Loss: -1.2407369729400357\n",
      "Epoch: 76     Loss: -1.242286361634218\n",
      "Epoch: 77     Loss: -1.244138629045204\n",
      "Epoch: 78     Loss: -1.2464164975596022\n",
      "Epoch: 79     Loss: -1.2485360848386797\n",
      "Epoch: 80     Loss: -1.2506639889151718\n",
      "Epoch: 81     Loss: -1.252596833213165\n",
      "Epoch: 82     Loss: -1.2547989562576791\n",
      "Epoch: 83     Loss: -1.2569767071989293\n",
      "Epoch: 84     Loss: -1.2590953217725982\n",
      "Epoch: 85     Loss: -1.2612223449901652\n",
      "Epoch: 86     Loss: -1.2633293603565512\n",
      "Epoch: 87     Loss: -1.2654857963286539\n",
      "Epoch: 88     Loss: -1.2675452243604355\n",
      "Epoch: 89     Loss: -1.2696781840570461\n",
      "Epoch: 90     Loss: -1.2715934027362763\n",
      "Epoch: 91     Loss: -1.2736978274156496\n",
      "Epoch: 92     Loss: -1.2753347907093522\n",
      "Epoch: 93     Loss: -1.277573835295727\n",
      "Epoch: 94     Loss: -1.279434002402727\n",
      "Epoch: 95     Loss: -1.2813317072934007\n",
      "Epoch: 96     Loss: -1.2826478277043927\n",
      "Epoch: 97     Loss: -1.284897337772641\n",
      "Epoch: 98     Loss: -1.2871810966128652\n",
      "Epoch: 99     Loss: -1.2892829943342001\n",
      "Epoch: 100     Loss: -1.2911808846495996\n",
      "Epoch: 101     Loss: -1.2923111745870846\n",
      "Epoch: 102     Loss: -1.2941923625475347\n",
      "Epoch: 103     Loss: -1.2962775371662296\n",
      "Epoch: 104     Loss: -1.298347268379983\n",
      "Epoch: 105     Loss: -1.3003755271191875\n",
      "Epoch: 106     Loss: -1.3020508810746767\n",
      "Epoch: 107     Loss: -1.3042016104363663\n",
      "Epoch: 108     Loss: -1.3062203364067424\n",
      "Epoch: 109     Loss: -1.3080574690166744\n",
      "Epoch: 110     Loss: -1.3093350585094994\n",
      "Epoch: 111     Loss: -1.3113610959908568\n",
      "Epoch: 112     Loss: -1.313445530883255\n",
      "Epoch: 113     Loss: -1.3153415042260863\n",
      "Epoch: 114     Loss: -1.3170705287687428\n",
      "Epoch: 115     Loss: -1.318439005406495\n",
      "Epoch: 116     Loss: -1.3197957736212775\n",
      "Epoch: 117     Loss: -1.3213346304839937\n",
      "Epoch: 118     Loss: -1.3230105062176234\n",
      "Epoch: 119     Loss: -1.3245455293820632\n",
      "Epoch: 120     Loss: -1.326064541032479\n",
      "Epoch: 121     Loss: -1.3274949057105305\n",
      "Epoch: 122     Loss: -1.3290396618138995\n",
      "Epoch: 123     Loss: -1.3306136448999422\n",
      "Epoch: 124     Loss: -1.3321971456475918\n",
      "Epoch: 125     Loss: -1.333708946840472\n",
      "Epoch: 126     Loss: -1.3352822265654114\n",
      "Epoch: 127     Loss: -1.3366835275061413\n",
      "Epoch: 128     Loss: -1.338348314859115\n",
      "Epoch: 129     Loss: -1.3398414853348122\n",
      "Epoch: 130     Loss: -1.3415044932228197\n",
      "Epoch: 131     Loss: -1.3428931842306255\n",
      "Epoch: 132     Loss: -1.3446659388274864\n",
      "Epoch: 133     Loss: -1.3463403401961098\n",
      "Epoch: 134     Loss: -1.3479509940886758\n",
      "Epoch: 135     Loss: -1.3491687830079855\n",
      "Epoch: 136     Loss: -1.3507933896909283\n",
      "Epoch: 137     Loss: -1.3524911671356108\n",
      "Epoch: 138     Loss: -1.3541842617211097\n",
      "Epoch: 139     Loss: -1.3556982152935362\n",
      "Epoch: 140     Loss: -1.3570088509151381\n",
      "Epoch: 141     Loss: -1.3580402548361652\n",
      "Epoch: 142     Loss: -1.3594765000325644\n",
      "Epoch: 143     Loss: -1.3610067847451663\n",
      "Epoch: 144     Loss: -1.362462231627458\n",
      "Epoch: 145     Loss: -1.3639013338056736\n",
      "Epoch: 146     Loss: -1.3652621762723733\n",
      "Epoch: 147     Loss: -1.3666330956223474\n",
      "Epoch: 148     Loss: -1.3680426039077647\n",
      "Epoch: 149     Loss: -1.3694641411301252\n",
      "Epoch: 150     Loss: -1.3708573402337183\n",
      "Epoch: 151     Loss: -1.372212497812505\n",
      "Epoch: 152     Loss: -1.3735840356630045\n",
      "Epoch: 153     Loss: -1.3750300695416382\n",
      "Epoch: 154     Loss: -1.3763192475452934\n",
      "Epoch: 155     Loss: -1.3775680352554405\n",
      "Epoch: 156     Loss: -1.3787762633730072\n",
      "Epoch: 157     Loss: -1.380235848159595\n",
      "Epoch: 158     Loss: -1.3816068735617522\n",
      "Epoch: 159     Loss: -1.382992329836476\n",
      "Epoch: 160     Loss: -1.3841410979125341\n",
      "Epoch: 161     Loss: -1.3856156112401334\n",
      "Epoch: 162     Loss: -1.3870486680692204\n",
      "Epoch: 163     Loss: -1.3883676647017111\n",
      "Epoch: 164     Loss: -1.389660548147147\n",
      "Epoch: 165     Loss: -1.3910357586090172\n",
      "Epoch: 166     Loss: -1.3922437625059836\n",
      "Epoch: 167     Loss: -1.3936370479520364\n",
      "Epoch: 168     Loss: -1.3949090458631674\n",
      "Epoch: 169     Loss: -1.396208991017717\n",
      "Epoch: 170     Loss: -1.3972313960750942\n",
      "Epoch: 171     Loss: -1.3985510261121847\n",
      "Epoch: 172     Loss: -1.3998750342474102\n",
      "Epoch: 173     Loss: -1.4011887090212762\n",
      "Epoch: 174     Loss: -1.402360103890753\n",
      "Epoch: 175     Loss: -1.4036251985855601\n",
      "Epoch: 176     Loss: -1.4046457366857021\n",
      "Epoch: 177     Loss: -1.4058898171000107\n",
      "Epoch: 178     Loss: -1.4071408398066185\n",
      "Epoch: 179     Loss: -1.4083995483185237\n",
      "Epoch: 180     Loss: -1.4095561405889856\n",
      "Epoch: 181     Loss: -1.4107702967484712\n",
      "Epoch: 182     Loss: -1.4118230306670705\n",
      "Epoch: 183     Loss: -1.4130010593699311\n",
      "Epoch: 184     Loss: -1.4140804588397906\n",
      "Epoch: 185     Loss: -1.4150688915110832\n",
      "Epoch: 186     Loss: -1.4160972736464958\n",
      "Epoch: 187     Loss: -1.4171216157407345\n",
      "Epoch: 188     Loss: -1.418180079463898\n",
      "Epoch: 189     Loss: -1.419246393593182\n",
      "Epoch: 190     Loss: -1.4203495076229753\n",
      "Epoch: 191     Loss: -1.4213800070245315\n",
      "Epoch: 192     Loss: -1.4223840103855696\n",
      "Epoch: 193     Loss: -1.4233919162539912\n",
      "Epoch: 194     Loss: -1.4245140915385615\n",
      "Epoch: 195     Loss: -1.425540418713044\n",
      "Epoch: 196     Loss: -1.4265788200660408\n",
      "Epoch: 197     Loss: -1.427529399063237\n",
      "Epoch: 198     Loss: -1.4285279103297368\n",
      "Epoch: 199     Loss: -1.4295594720850275\n",
      "Epoch: 200     Loss: -1.4306574021703102\n",
      "Epoch: 201     Loss: -1.4317004633618398\n",
      "Epoch: 202     Loss: -1.4327596028610134\n",
      "Epoch: 203     Loss: -1.4337570410131413\n",
      "Epoch: 204     Loss: -1.4347123108671935\n",
      "Epoch: 205     Loss: -1.4357273059287017\n",
      "Epoch: 206     Loss: -1.4368040026030227\n",
      "Epoch: 207     Loss: -1.4378401758810764\n",
      "Epoch: 208     Loss: -1.4388892087976768\n",
      "Epoch: 209     Loss: -1.439872075726781\n",
      "Epoch: 210     Loss: -1.4408287603435934\n",
      "Epoch: 211     Loss: -1.441811308768008\n",
      "Epoch: 212     Loss: -1.4428419977884974\n",
      "Epoch: 213     Loss: -1.443813044577109\n",
      "Epoch: 214     Loss: -1.4447247263050194\n",
      "Epoch: 215     Loss: -1.4456799711836827\n",
      "Epoch: 216     Loss: -1.4466773451917478\n",
      "Epoch: 217     Loss: -1.44761303584968\n",
      "Epoch: 218     Loss: -1.4484780451948671\n",
      "Epoch: 219     Loss: -1.4493886890044836\n",
      "Epoch: 220     Loss: -1.4503167056190358\n",
      "Epoch: 221     Loss: -1.4512282455459549\n",
      "Epoch: 222     Loss: -1.4520650203607175\n",
      "Epoch: 223     Loss: -1.4529949940145315\n",
      "Epoch: 224     Loss: -1.4539128135489972\n",
      "Epoch: 225     Loss: -1.4548122032401736\n",
      "Epoch: 226     Loss: -1.4556173665429477\n",
      "Epoch: 227     Loss: -1.4565435063113819\n",
      "Epoch: 228     Loss: -1.4574532888095706\n",
      "Epoch: 229     Loss: -1.458365158749159\n",
      "Epoch: 230     Loss: -1.459148896842904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 231     Loss: -1.4600604199429008\n",
      "Epoch: 232     Loss: -1.460991622350035\n",
      "Epoch: 233     Loss: -1.4619134662680875\n",
      "Epoch: 234     Loss: -1.4627613988415564\n",
      "Epoch: 235     Loss: -1.463509572383842\n",
      "Epoch: 236     Loss: -1.4642974766890957\n",
      "Epoch: 237     Loss: -1.4651819053387443\n",
      "Epoch: 238     Loss: -1.466053280117299\n",
      "Epoch: 239     Loss: -1.46693419778102\n",
      "Epoch: 240     Loss: -1.4677209012021597\n",
      "Epoch: 241     Loss: -1.4686132207130067\n",
      "Epoch: 242     Loss: -1.4694955174036066\n",
      "Epoch: 243     Loss: -1.4703790896098274\n",
      "Epoch: 244     Loss: -1.4712171887796066\n",
      "Epoch: 245     Loss: -1.472035341027411\n",
      "Epoch: 246     Loss: -1.472820380614588\n",
      "Epoch: 247     Loss: -1.4736351952681062\n",
      "Epoch: 248     Loss: -1.474353229091482\n",
      "Epoch: 249     Loss: -1.4750878301420811\n",
      "Epoch: 250     Loss: -1.4758190598602372\n",
      "Epoch: 251     Loss: -1.476599546131173\n",
      "Epoch: 252     Loss: -1.4773085047421821\n",
      "Epoch: 253     Loss: -1.4779722897869003\n",
      "Epoch: 254     Loss: -1.4787156233464873\n",
      "Epoch: 255     Loss: -1.4795276626558804\n",
      "Epoch: 256     Loss: -1.4803056324327462\n",
      "Epoch: 257     Loss: -1.4810170906645153\n",
      "Epoch: 258     Loss: -1.4817821836872922\n",
      "Epoch: 259     Loss: -1.48256263047127\n",
      "Epoch: 260     Loss: -1.4833378351691824\n",
      "Epoch: 261     Loss: -1.4841138887138858\n",
      "Epoch: 262     Loss: -1.4848566250477715\n",
      "Epoch: 263     Loss: -1.4855714165496967\n",
      "Epoch: 264     Loss: -1.4863238463787292\n",
      "Epoch: 265     Loss: -1.4870920470445312\n",
      "Epoch: 266     Loss: -1.4878376467607766\n",
      "Epoch: 267     Loss: -1.4885545671670881\n",
      "Epoch: 268     Loss: -1.4892966076493346\n",
      "Epoch: 269     Loss: -1.4900580133936536\n",
      "Epoch: 270     Loss: -1.4907606405932845\n",
      "Epoch: 271     Loss: -1.4914310166174085\n",
      "Epoch: 272     Loss: -1.492100759379919\n",
      "Epoch: 273     Loss: -1.4928020390142815\n",
      "Epoch: 274     Loss: -1.4934957095368093\n",
      "Epoch: 275     Loss: -1.4941663428531862\n",
      "Epoch: 276     Loss: -1.494857989365124\n",
      "Epoch: 277     Loss: -1.4954980358998582\n",
      "Epoch: 278     Loss: -1.496197636504635\n",
      "Epoch: 279     Loss: -1.4968245943484937\n",
      "Epoch: 280     Loss: -1.4975457841936866\n",
      "Epoch: 281     Loss: -1.4982253941790649\n",
      "Epoch: 282     Loss: -1.4989216588223382\n",
      "Epoch: 283     Loss: -1.4994902504764365\n",
      "Epoch: 284     Loss: -1.5002042929388792\n",
      "Epoch: 285     Loss: -1.5009110473950866\n",
      "Epoch: 286     Loss: -1.5016120908043908\n",
      "Epoch: 287     Loss: -1.5022331759072889\n",
      "Epoch: 288     Loss: -1.5029186518385325\n",
      "Epoch: 289     Loss: -1.503603488970547\n",
      "Epoch: 290     Loss: -1.5042960326819355\n",
      "Epoch: 291     Loss: -1.504974566665182\n",
      "Epoch: 292     Loss: -1.5056243196506744\n",
      "Epoch: 293     Loss: -1.506279926840992\n",
      "Epoch: 294     Loss: -1.50695933287361\n",
      "Epoch: 295     Loss: -1.5075803020547858\n",
      "Epoch: 296     Loss: -1.5081835883856656\n",
      "Epoch: 297     Loss: -1.5088021012792527\n",
      "Epoch: 298     Loss: -1.5094690756358458\n",
      "Epoch: 299     Loss: -1.510102181872117\n",
      "Epoch: 300     Loss: -1.5107447422951483\n",
      "Epoch: 301     Loss: -1.5113616634541052\n",
      "Epoch: 302     Loss: -1.5119624082808272\n",
      "Epoch: 303     Loss: -1.5126125019470449\n",
      "Epoch: 304     Loss: -1.5132589590677588\n",
      "Epoch: 305     Loss: -1.5138941684477556\n",
      "Epoch: 306     Loss: -1.5144260275007078\n",
      "Epoch: 307     Loss: -1.5150547062707462\n",
      "Epoch: 308     Loss: -1.515652131214046\n",
      "Epoch: 309     Loss: -1.5162847397406796\n",
      "Epoch: 310     Loss: -1.5168312526772672\n",
      "Epoch: 311     Loss: -1.5174579679723703\n",
      "Epoch: 312     Loss: -1.518037006167673\n",
      "Epoch: 313     Loss: -1.518627151098496\n",
      "Epoch: 314     Loss: -1.5191564812691956\n",
      "Epoch: 315     Loss: -1.519745919016402\n",
      "Epoch: 316     Loss: -1.5203426555656492\n",
      "Epoch: 317     Loss: -1.5208957757137593\n",
      "Epoch: 318     Loss: -1.521481076995225\n",
      "Epoch: 319     Loss: -1.5220234600109934\n",
      "Epoch: 320     Loss: -1.5226275716271944\n",
      "Epoch: 321     Loss: -1.5231638212411207\n",
      "Epoch: 322     Loss: -1.523785192956991\n",
      "Epoch: 323     Loss: -1.5243657058744373\n",
      "Epoch: 324     Loss: -1.5249533319359319\n",
      "Epoch: 325     Loss: -1.5254312655247213\n",
      "Epoch: 326     Loss: -1.526029762859959\n",
      "Epoch: 327     Loss: -1.5266134009201433\n",
      "Epoch: 328     Loss: -1.5272106789442919\n",
      "Epoch: 329     Loss: -1.5277575604294436\n",
      "Epoch: 330     Loss: -1.528341290344618\n",
      "Epoch: 331     Loss: -1.528898589105237\n",
      "Epoch: 332     Loss: -1.5294824862752383\n",
      "Epoch: 333     Loss: -1.5300457262214844\n",
      "Epoch: 334     Loss: -1.5306185604436429\n",
      "Epoch: 335     Loss: -1.5311815833020777\n",
      "Epoch: 336     Loss: -1.5317318563926619\n",
      "Epoch: 337     Loss: -1.5322975456359953\n",
      "Epoch: 338     Loss: -1.5328338874987686\n",
      "Epoch: 339     Loss: -1.533343885111621\n",
      "Epoch: 340     Loss: -1.5337799385332105\n",
      "Epoch: 341     Loss: -1.5343057764207464\n",
      "Epoch: 342     Loss: -1.5348623393881244\n",
      "Epoch: 343     Loss: -1.5354022724648897\n",
      "Epoch: 344     Loss: -1.5359440785407008\n",
      "Epoch: 345     Loss: -1.5364531460792432\n",
      "Epoch: 346     Loss: -1.536976028092463\n",
      "Epoch: 347     Loss: -1.537463483976313\n",
      "Epoch: 348     Loss: -1.5380187110980177\n",
      "Epoch: 349     Loss: -1.538547655710151\n",
      "Epoch: 350     Loss: -1.5390945134009897\n",
      "Epoch: 351     Loss: -1.5395741167637462\n",
      "Epoch: 352     Loss: -1.54013838045587\n",
      "Epoch: 353     Loss: -1.540686776861469\n",
      "Epoch: 354     Loss: -1.541240742246591\n",
      "Epoch: 355     Loss: -1.5417602538670674\n",
      "Epoch: 356     Loss: -1.5423162429433463\n",
      "Epoch: 357     Loss: -1.542848106247373\n",
      "Epoch: 358     Loss: -1.5433950620992565\n",
      "Epoch: 359     Loss: -1.5439042388727764\n",
      "Epoch: 360     Loss: -1.5443586655805333\n",
      "Epoch: 361     Loss: -1.544851099412481\n",
      "Epoch: 362     Loss: -1.5453466605622186\n",
      "Epoch: 363     Loss: -1.545820744179334\n",
      "Epoch: 364     Loss: -1.5462143402883648\n",
      "Epoch: 365     Loss: -1.5466959639475024\n",
      "Epoch: 366     Loss: -1.5472001100322172\n",
      "Epoch: 367     Loss: -1.547698140061306\n",
      "Epoch: 368     Loss: -1.5481600224642573\n",
      "Epoch: 369     Loss: -1.5486616317541353\n",
      "Epoch: 370     Loss: -1.5491584522590627\n",
      "Epoch: 371     Loss: -1.5496508181953492\n",
      "Epoch: 372     Loss: -1.5501224651671026\n",
      "Epoch: 373     Loss: -1.550595185713323\n",
      "Epoch: 374     Loss: -1.5510362595948615\n",
      "Epoch: 375     Loss: -1.551496344430609\n",
      "Epoch: 376     Loss: -1.551920731593654\n",
      "Epoch: 377     Loss: -1.5524141729876328\n",
      "Epoch: 378     Loss: -1.5529075103560144\n",
      "Epoch: 379     Loss: -1.5534085886202618\n",
      "Epoch: 380     Loss: -1.5538921176026852\n",
      "Epoch: 381     Loss: -1.5543840025392999\n",
      "Epoch: 382     Loss: -1.554854035452253\n",
      "Epoch: 383     Loss: -1.5553382196387597\n",
      "Epoch: 384     Loss: -1.5557932567125574\n",
      "Epoch: 385     Loss: -1.5562707216210878\n",
      "Epoch: 386     Loss: -1.5567025181687346\n",
      "Epoch: 387     Loss: -1.5571961927090954\n",
      "Epoch: 388     Loss: -1.5576759582473474\n",
      "Epoch: 389     Loss: -1.5581666105952099\n",
      "Epoch: 390     Loss: -1.5586215535315788\n",
      "Epoch: 391     Loss: -1.5591042279865646\n",
      "Epoch: 392     Loss: -1.5595475837932555\n",
      "Epoch: 393     Loss: -1.5600300866021244\n",
      "Epoch: 394     Loss: -1.560479571966644\n",
      "Epoch: 395     Loss: -1.560954935527608\n",
      "Epoch: 396     Loss: -1.561386622236982\n",
      "Epoch: 397     Loss: -1.5618462494109209\n",
      "Epoch: 398     Loss: -1.562254994150911\n",
      "Epoch: 399     Loss: -1.562680025882739\n",
      "Epoch: 400     Loss: -1.5630996216646629\n",
      "Epoch: 401     Loss: -1.563533485922459\n",
      "Epoch: 402     Loss: -1.5639451731206455\n",
      "Epoch: 403     Loss: -1.5643958820562267\n",
      "Epoch: 404     Loss: -1.5648322554534322\n",
      "Epoch: 405     Loss: -1.5652792686558414\n",
      "Epoch: 406     Loss: -1.5656846496526375\n",
      "Epoch: 407     Loss: -1.5661377687380829\n",
      "Epoch: 408     Loss: -1.5665626485371358\n",
      "Epoch: 409     Loss: -1.5670124585733842\n",
      "Epoch: 410     Loss: -1.5674532908572318\n",
      "Epoch: 411     Loss: -1.5679025830238007\n",
      "Epoch: 412     Loss: -1.5683428394818861\n",
      "Epoch: 413     Loss: -1.568788156477619\n",
      "Epoch: 414     Loss: -1.5692262092692435\n",
      "Epoch: 415     Loss: -1.5696539163454613\n",
      "Epoch: 416     Loss: -1.5700684372171325\n",
      "Epoch: 417     Loss: -1.570459435064732\n",
      "Epoch: 418     Loss: -1.5708690782178292\n",
      "Epoch: 419     Loss: -1.571275336458435\n",
      "Epoch: 420     Loss: -1.571705786995765\n",
      "Epoch: 421     Loss: -1.572118595235361\n",
      "Epoch: 422     Loss: -1.5725388573403911\n",
      "Epoch: 423     Loss: -1.5729292454831085\n",
      "Epoch: 424     Loss: -1.5732970669395479\n",
      "Epoch: 425     Loss: -1.5736875391510627\n",
      "Epoch: 426     Loss: -1.5740884975270435\n",
      "Epoch: 427     Loss: -1.5744820626320122\n",
      "Epoch: 428     Loss: -1.5748579061459014\n",
      "Epoch: 429     Loss: -1.5752604411254227\n",
      "Epoch: 430     Loss: -1.5756531972725962\n",
      "Epoch: 431     Loss: -1.5760581102225237\n",
      "Epoch: 432     Loss: -1.576425733484818\n",
      "Epoch: 433     Loss: -1.5768419402968687\n",
      "Epoch: 434     Loss: -1.5772435397194027\n",
      "Epoch: 435     Loss: -1.5776591084219789\n",
      "Epoch: 436     Loss: -1.578047339035804\n",
      "Epoch: 437     Loss: -1.5784558787001133\n",
      "Epoch: 438     Loss: -1.5788087623181206\n",
      "Epoch: 439     Loss: -1.5792148494878357\n",
      "Epoch: 440     Loss: -1.5795997519071678\n",
      "Epoch: 441     Loss: -1.5800107187392631\n",
      "Epoch: 442     Loss: -1.5803910853294898\n",
      "Epoch: 443     Loss: -1.5808004806292866\n",
      "Epoch: 444     Loss: -1.5811837048368504\n",
      "Epoch: 445     Loss: -1.5815943287990286\n",
      "Epoch: 446     Loss: -1.5819832229747615\n",
      "Epoch: 447     Loss: -1.5823774305058054\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 448     Loss: -1.5827308585676028\n",
      "Epoch: 449     Loss: -1.5830351472188757\n",
      "Epoch: 450     Loss: -1.5834074041452282\n",
      "Epoch: 451     Loss: -1.5838073908778074\n",
      "Epoch: 452     Loss: -1.5841939403511296\n",
      "Epoch: 453     Loss: -1.5845871821132715\n",
      "Epoch: 454     Loss: -1.5849646249516443\n",
      "Epoch: 455     Loss: -1.5853447779139203\n",
      "Epoch: 456     Loss: -1.5857184294225544\n",
      "Epoch: 457     Loss: -1.5860781585834867\n",
      "Epoch: 458     Loss: -1.5864589250147803\n",
      "Epoch: 459     Loss: -1.5868496362484918\n",
      "Epoch: 460     Loss: -1.5872265510573238\n",
      "Epoch: 461     Loss: -1.5875951515135878\n",
      "Epoch: 462     Loss: -1.587959122749261\n",
      "Epoch: 463     Loss: -1.5883234917187652\n",
      "Epoch: 464     Loss: -1.5886857863698847\n",
      "Epoch: 465     Loss: -1.589045389804763\n",
      "Epoch: 466     Loss: -1.5894083349202541\n",
      "Epoch: 467     Loss: -1.589772058801693\n",
      "Epoch: 468     Loss: -1.5901342092269741\n",
      "Epoch: 469     Loss: -1.5905016903503746\n",
      "Epoch: 470     Loss: -1.5908659570956056\n",
      "Epoch: 471     Loss: -1.5912367096931852\n",
      "Epoch: 472     Loss: -1.5915919885677374\n",
      "Epoch: 473     Loss: -1.5919615656257207\n",
      "Epoch: 474     Loss: -1.5922983752842914\n",
      "Epoch: 475     Loss: -1.592686014308182\n",
      "Epoch: 476     Loss: -1.5930543655704892\n",
      "Epoch: 477     Loss: -1.5934086953116946\n",
      "Epoch: 478     Loss: -1.5937558929480071\n",
      "Epoch: 479     Loss: -1.5941141830914916\n",
      "Epoch: 480     Loss: -1.5944635680390546\n",
      "Epoch: 481     Loss: -1.5948263026998857\n",
      "Epoch: 482     Loss: -1.5951810256867354\n",
      "Epoch: 483     Loss: -1.5955423805111242\n",
      "Epoch: 484     Loss: -1.5959015111826809\n",
      "Epoch: 485     Loss: -1.5962666539651342\n",
      "Epoch: 486     Loss: -1.596626391248597\n",
      "Epoch: 487     Loss: -1.596986001055576\n",
      "Epoch: 488     Loss: -1.597339789850282\n",
      "Epoch: 489     Loss: -1.597678156644163\n",
      "Epoch: 490     Loss: -1.5980358889762194\n",
      "Epoch: 491     Loss: -1.5983857658817702\n",
      "Epoch: 492     Loss: -1.5987405988061971\n",
      "Epoch: 493     Loss: -1.599078983585951\n",
      "Epoch: 494     Loss: -1.5994290847689674\n",
      "Epoch: 495     Loss: -1.5997510353614772\n",
      "Epoch: 496     Loss: -1.6000889457433185\n",
      "Epoch: 497     Loss: -1.6003906711718305\n",
      "Epoch: 498     Loss: -1.600731359418401\n",
      "Epoch: 499     Loss: -1.6010498775000772\n",
      "Epoch: 500     Loss: -1.601400467250328\n",
      "Epoch: 501     Loss: -1.6017364594975236\n",
      "Epoch: 502     Loss: -1.602092309496481\n",
      "Epoch: 503     Loss: -1.602433387000572\n",
      "Epoch: 504     Loss: -1.6027864800021396\n",
      "Epoch: 505     Loss: -1.603117345642158\n",
      "Epoch: 506     Loss: -1.6034630453796381\n",
      "Epoch: 507     Loss: -1.6037785708732584\n",
      "Epoch: 508     Loss: -1.604093093845612\n",
      "Epoch: 509     Loss: -1.6043964750761452\n",
      "Epoch: 510     Loss: -1.6046638638329822\n",
      "Epoch: 511     Loss: -1.6049316056739453\n",
      "Epoch: 512     Loss: -1.605224334017398\n",
      "Epoch: 513     Loss: -1.6055404626110714\n",
      "Epoch: 514     Loss: -1.605833635648665\n",
      "Epoch: 515     Loss: -1.606165413578311\n",
      "Epoch: 516     Loss: -1.6064892810738045\n",
      "Epoch: 517     Loss: -1.6068200862258717\n",
      "Epoch: 518     Loss: -1.6071486347649215\n",
      "Epoch: 519     Loss: -1.607482960329192\n",
      "Epoch: 520     Loss: -1.607815573552243\n",
      "Epoch: 521     Loss: -1.6081449958375191\n",
      "Epoch: 522     Loss: -1.60847981158625\n",
      "Epoch: 523     Loss: -1.6088172910643574\n",
      "Epoch: 524     Loss: -1.6091465992839769\n",
      "Epoch: 525     Loss: -1.609472834795942\n",
      "Epoch: 526     Loss: -1.6097945620783016\n",
      "Epoch: 527     Loss: -1.610118842073503\n",
      "Epoch: 528     Loss: -1.6104293246727273\n",
      "Epoch: 529     Loss: -1.6107296042765376\n",
      "Epoch: 530     Loss: -1.6110362333079709\n",
      "Epoch: 531     Loss: -1.611350455605028\n",
      "Epoch: 532     Loss: -1.6116645015898992\n",
      "Epoch: 533     Loss: -1.6119802377517072\n",
      "Epoch: 534     Loss: -1.6122991367620567\n",
      "Epoch: 535     Loss: -1.612616843833011\n",
      "Epoch: 536     Loss: -1.6129400377137324\n",
      "Epoch: 537     Loss: -1.6132606620053975\n",
      "Epoch: 538     Loss: -1.6135851437667037\n",
      "Epoch: 539     Loss: -1.6139016639689774\n",
      "Epoch: 540     Loss: -1.6142022388960098\n",
      "Epoch: 541     Loss: -1.6145321803990467\n",
      "Epoch: 542     Loss: -1.614860238150746\n",
      "Epoch: 543     Loss: -1.6151853937782301\n",
      "Epoch: 544     Loss: -1.615513194109725\n",
      "Epoch: 545     Loss: -1.6158306453751132\n",
      "Epoch: 546     Loss: -1.6161538261594344\n",
      "Epoch: 547     Loss: -1.6164632616929104\n",
      "Epoch: 548     Loss: -1.6167811792037627\n",
      "Epoch: 549     Loss: -1.6170775583498584\n",
      "Epoch: 550     Loss: -1.6173987263095069\n",
      "Epoch: 551     Loss: -1.6176872015491273\n",
      "Epoch: 552     Loss: -1.617968432997212\n",
      "Epoch: 553     Loss: -1.618209163434288\n",
      "Epoch: 554     Loss: -1.6185143970056612\n",
      "Epoch: 555     Loss: -1.6188175479882492\n",
      "Epoch: 556     Loss: -1.6191386653150384\n",
      "Epoch: 557     Loss: -1.6194496918075993\n",
      "Epoch: 558     Loss: -1.6197693745437765\n",
      "Epoch: 559     Loss: -1.620073253473486\n",
      "Epoch: 560     Loss: -1.6203801863280427\n",
      "Epoch: 561     Loss: -1.620655370953772\n",
      "Epoch: 562     Loss: -1.6209488852964336\n",
      "Epoch: 563     Loss: -1.6212139879981167\n",
      "Epoch: 564     Loss: -1.6214961149198743\n",
      "Epoch: 565     Loss: -1.6217625254310317\n",
      "Epoch: 566     Loss: -1.6220446614861546\n",
      "Epoch: 567     Loss: -1.6223260762993246\n",
      "Epoch: 568     Loss: -1.6226035907315843\n",
      "Epoch: 569     Loss: -1.622865513826651\n",
      "Epoch: 570     Loss: -1.62313824507835\n",
      "Epoch: 571     Loss: -1.6234213017824315\n",
      "Epoch: 572     Loss: -1.6236973262755006\n",
      "Epoch: 573     Loss: -1.6239951654166371\n",
      "Epoch: 574     Loss: -1.6242826598808005\n",
      "Epoch: 575     Loss: -1.6245685048847613\n",
      "Epoch: 576     Loss: -1.6248608500734814\n",
      "Epoch: 577     Loss: -1.6251549058734545\n",
      "Epoch: 578     Loss: -1.6254423734915109\n",
      "Epoch: 579     Loss: -1.625719453149872\n",
      "Epoch: 580     Loss: -1.6260178324288934\n",
      "Epoch: 581     Loss: -1.626316606524532\n",
      "Epoch: 582     Loss: -1.6266166555166144\n",
      "Epoch: 583     Loss: -1.6269119684063487\n",
      "Epoch: 584     Loss: -1.627209547185135\n",
      "Epoch: 585     Loss: -1.6274964531464835\n",
      "Epoch: 586     Loss: -1.6277879595561155\n",
      "Epoch: 587     Loss: -1.6280708391604055\n",
      "Epoch: 588     Loss: -1.628368523810003\n",
      "Epoch: 589     Loss: -1.628657149945024\n",
      "Epoch: 590     Loss: -1.6289518768363662\n",
      "Epoch: 591     Loss: -1.6292305845609742\n",
      "Epoch: 592     Loss: -1.6295133725398383\n",
      "Epoch: 593     Loss: -1.6297666366649484\n",
      "Epoch: 594     Loss: -1.6300194559771424\n",
      "Epoch: 595     Loss: -1.6302519808293798\n",
      "Epoch: 596     Loss: -1.6305087737225352\n",
      "Epoch: 597     Loss: -1.6307692884455232\n",
      "Epoch: 598     Loss: -1.6310425142674723\n",
      "Epoch: 599     Loss: -1.6313076734606773\n",
      "Epoch: 600     Loss: -1.6315809393497118\n",
      "Epoch: 601     Loss: -1.6318536285431504\n",
      "Epoch: 602     Loss: -1.6321324177890926\n",
      "Epoch: 603     Loss: -1.6324130257916636\n",
      "Epoch: 604     Loss: -1.6326982447169767\n",
      "Epoch: 605     Loss: -1.6329833865106353\n",
      "Epoch: 606     Loss: -1.6332681648211813\n",
      "Epoch: 607     Loss: -1.6335454534556209\n",
      "Epoch: 608     Loss: -1.6337971795614576\n",
      "Epoch: 609     Loss: -1.6340701417403098\n",
      "Epoch: 610     Loss: -1.6343537334957634\n",
      "Epoch: 611     Loss: -1.634628371457974\n",
      "Epoch: 612     Loss: -1.6348900076803827\n",
      "Epoch: 613     Loss: -1.6351495129660116\n",
      "Epoch: 614     Loss: -1.6354137007640088\n",
      "Epoch: 615     Loss: -1.6356600356890691\n",
      "Epoch: 616     Loss: -1.635922009775018\n",
      "Epoch: 617     Loss: -1.6361809812319767\n",
      "Epoch: 618     Loss: -1.6364479716291682\n",
      "Epoch: 619     Loss: -1.636716324527617\n",
      "Epoch: 620     Loss: -1.6369894702218324\n",
      "Epoch: 621     Loss: -1.637261457863855\n",
      "Epoch: 622     Loss: -1.637529491711274\n",
      "Epoch: 623     Loss: -1.6377953069136637\n",
      "Epoch: 624     Loss: -1.6380425204382787\n",
      "Epoch: 625     Loss: -1.6383090754013347\n",
      "Epoch: 626     Loss: -1.6385564086017932\n",
      "Epoch: 627     Loss: -1.6388133047171447\n",
      "Epoch: 628     Loss: -1.6390562105825144\n",
      "Epoch: 629     Loss: -1.6393161500826876\n",
      "Epoch: 630     Loss: -1.6395695860270547\n",
      "Epoch: 631     Loss: -1.6398378866338712\n",
      "Epoch: 632     Loss: -1.6401017113284113\n",
      "Epoch: 633     Loss: -1.640375983122841\n",
      "Epoch: 634     Loss: -1.6406385162785584\n",
      "Epoch: 635     Loss: -1.6409022663494097\n",
      "Epoch: 636     Loss: -1.6411466916745558\n",
      "Epoch: 637     Loss: -1.6414213782556293\n",
      "Epoch: 638     Loss: -1.641693387452222\n",
      "Epoch: 639     Loss: -1.6419715756022553\n",
      "Epoch: 640     Loss: -1.6422466439674157\n",
      "Epoch: 641     Loss: -1.6425210816949845\n",
      "Epoch: 642     Loss: -1.6427810110121237\n",
      "Epoch: 643     Loss: -1.643046481427982\n",
      "Epoch: 644     Loss: -1.6433023862725578\n",
      "Epoch: 645     Loss: -1.6435619365580425\n",
      "Epoch: 646     Loss: -1.6438200086925174\n",
      "Epoch: 647     Loss: -1.644072604817756\n",
      "Epoch: 648     Loss: -1.6443214763025722\n",
      "Epoch: 649     Loss: -1.6445676694550009\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 650     Loss: -1.6448206193796813\n",
      "Epoch: 651     Loss: -1.6450648834344583\n",
      "Epoch: 652     Loss: -1.645315989281573\n",
      "Epoch: 653     Loss: -1.6455559846813967\n",
      "Epoch: 654     Loss: -1.6457975547862385\n",
      "Epoch: 655     Loss: -1.646029530264091\n",
      "Epoch: 656     Loss: -1.6462705523971783\n",
      "Epoch: 657     Loss: -1.646495924619402\n",
      "Epoch: 658     Loss: -1.646741699479225\n",
      "Epoch: 659     Loss: -1.6469811301945625\n",
      "Epoch: 660     Loss: -1.6472358802256588\n",
      "Epoch: 661     Loss: -1.6474860874163035\n",
      "Epoch: 662     Loss: -1.647745884691096\n",
      "Epoch: 663     Loss: -1.6479990692895312\n",
      "Epoch: 664     Loss: -1.6482516445003979\n",
      "Epoch: 665     Loss: -1.6484870688226716\n",
      "Epoch: 666     Loss: -1.6486984956373836\n",
      "Epoch: 667     Loss: -1.6489372898552908\n",
      "Epoch: 668     Loss: -1.6491963865757173\n",
      "Epoch: 669     Loss: -1.6494476881575058\n",
      "Epoch: 670     Loss: -1.649694308385193\n",
      "Epoch: 671     Loss: -1.649926771614663\n",
      "Epoch: 672     Loss: -1.650175822922495\n",
      "Epoch: 673     Loss: -1.650421657541571\n",
      "Epoch: 674     Loss: -1.6506741839910655\n",
      "Epoch: 675     Loss: -1.6509214437903998\n",
      "Epoch: 676     Loss: -1.6511699465034322\n",
      "Epoch: 677     Loss: -1.65140376521923\n",
      "Epoch: 678     Loss: -1.6516392671521214\n",
      "Epoch: 679     Loss: -1.6518597991226946\n",
      "Epoch: 680     Loss: -1.6521032630444161\n",
      "Epoch: 681     Loss: -1.6523365084365267\n",
      "Epoch: 682     Loss: -1.6525781520718081\n",
      "Epoch: 683     Loss: -1.6528111656533875\n",
      "Epoch: 684     Loss: -1.653047813729989\n",
      "Epoch: 685     Loss: -1.6532717811737898\n",
      "Epoch: 686     Loss: -1.6535061211211481\n",
      "Epoch: 687     Loss: -1.6537380537484871\n",
      "Epoch: 688     Loss: -1.6539765719631894\n",
      "Epoch: 689     Loss: -1.6542131745051312\n",
      "Epoch: 690     Loss: -1.6544486310520152\n",
      "Epoch: 691     Loss: -1.6546813690249311\n",
      "Epoch: 692     Loss: -1.6549102045525748\n",
      "Epoch: 693     Loss: -1.6551416102718912\n",
      "Epoch: 694     Loss: -1.655374723677348\n",
      "Epoch: 695     Loss: -1.6556188243563241\n",
      "Epoch: 696     Loss: -1.6558571511001585\n",
      "Epoch: 697     Loss: -1.6561009021656534\n",
      "Epoch: 698     Loss: -1.6563436778810907\n",
      "Epoch: 699     Loss: -1.6565910627546823\n",
      "Epoch: 700     Loss: -1.656832970776411\n",
      "Epoch: 701     Loss: -1.6570781334530569\n",
      "Epoch: 702     Loss: -1.6573163141056588\n",
      "Epoch: 703     Loss: -1.6575546671405375\n",
      "Epoch: 704     Loss: -1.6577828245218975\n",
      "Epoch: 705     Loss: -1.6580120024842753\n",
      "Epoch: 706     Loss: -1.658236605745706\n",
      "Epoch: 707     Loss: -1.658467301110571\n",
      "Epoch: 708     Loss: -1.6586921309205365\n",
      "Epoch: 709     Loss: -1.6589219782855558\n",
      "Epoch: 710     Loss: -1.6591471645233995\n",
      "Epoch: 711     Loss: -1.6593749171241348\n",
      "Epoch: 712     Loss: -1.659589204176756\n",
      "Epoch: 713     Loss: -1.6598256699532414\n",
      "Epoch: 714     Loss: -1.6600573328910895\n",
      "Epoch: 715     Loss: -1.6602957746951872\n",
      "Epoch: 716     Loss: -1.6605308566172428\n",
      "Epoch: 717     Loss: -1.660767119957381\n",
      "Epoch: 718     Loss: -1.6609942496351042\n",
      "Epoch: 719     Loss: -1.6612189677838136\n",
      "Epoch: 720     Loss: -1.6614297751455898\n",
      "Epoch: 721     Loss: -1.6616373745082855\n",
      "Epoch: 722     Loss: -1.661848962218376\n",
      "Epoch: 723     Loss: -1.6620673676504452\n",
      "Epoch: 724     Loss: -1.662283939022423\n",
      "Epoch: 725     Loss: -1.6625008891699866\n",
      "Epoch: 726     Loss: -1.6627211126901233\n",
      "Epoch: 727     Loss: -1.662941824653824\n",
      "Epoch: 728     Loss: -1.6631639879610465\n",
      "Epoch: 729     Loss: -1.663387845011504\n",
      "Epoch: 730     Loss: -1.663612555455397\n",
      "Epoch: 731     Loss: -1.6638403265466186\n",
      "Epoch: 732     Loss: -1.6640666626681562\n",
      "Epoch: 733     Loss: -1.664291726235913\n",
      "Epoch: 734     Loss: -1.6645019823337683\n",
      "Epoch: 735     Loss: -1.6647223917815102\n",
      "Epoch: 736     Loss: -1.6649295884180149\n",
      "Epoch: 737     Loss: -1.6651522515844708\n",
      "Epoch: 738     Loss: -1.665362410906579\n",
      "Epoch: 739     Loss: -1.6655864942564382\n",
      "Epoch: 740     Loss: -1.6657978529074964\n",
      "Epoch: 741     Loss: -1.6660225423621462\n",
      "Epoch: 742     Loss: -1.666235998928839\n",
      "Epoch: 743     Loss: -1.666465024081097\n",
      "Epoch: 744     Loss: -1.6666859842447335\n",
      "Epoch: 745     Loss: -1.6669145882727379\n",
      "Epoch: 746     Loss: -1.6671337012859104\n",
      "Epoch: 747     Loss: -1.6673579390493176\n",
      "Epoch: 748     Loss: -1.667566446489984\n",
      "Epoch: 749     Loss: -1.6677779074422836\n",
      "Epoch: 750     Loss: -1.6679934560012128\n",
      "Epoch: 751     Loss: -1.6682091976694686\n",
      "Epoch: 752     Loss: -1.6684220682809534\n",
      "Epoch: 753     Loss: -1.6686333179113422\n",
      "Epoch: 754     Loss: -1.6688458648382172\n",
      "Epoch: 755     Loss: -1.6690467184061897\n",
      "Epoch: 756     Loss: -1.669256151414422\n",
      "Epoch: 757     Loss: -1.6694521893463259\n",
      "Epoch: 758     Loss: -1.6696661208405281\n",
      "Epoch: 759     Loss: -1.669869244807252\n",
      "Epoch: 760     Loss: -1.6700869536224296\n",
      "Epoch: 761     Loss: -1.670294890890889\n",
      "Epoch: 762     Loss: -1.670510667378211\n",
      "Epoch: 763     Loss: -1.6707198472890612\n",
      "Epoch: 764     Loss: -1.6709355177623706\n",
      "Epoch: 765     Loss: -1.6711403882149367\n",
      "Epoch: 766     Loss: -1.671355577189194\n",
      "Epoch: 767     Loss: -1.6715614113240964\n",
      "Epoch: 768     Loss: -1.6717794520480662\n",
      "Epoch: 769     Loss: -1.671991141912554\n",
      "Epoch: 770     Loss: -1.6722090679395842\n",
      "Epoch: 771     Loss: -1.6724221612049808\n",
      "Epoch: 772     Loss: -1.6726384871761213\n",
      "Epoch: 773     Loss: -1.672852892206764\n",
      "Epoch: 774     Loss: -1.6730659749415417\n",
      "Epoch: 775     Loss: -1.6732782163944742\n",
      "Epoch: 776     Loss: -1.6734884068005833\n",
      "Epoch: 777     Loss: -1.6736992589535458\n",
      "Epoch: 778     Loss: -1.6739063171448705\n",
      "Epoch: 779     Loss: -1.674112684042898\n",
      "Epoch: 780     Loss: -1.6743139991596416\n",
      "Epoch: 781     Loss: -1.6745196391197041\n",
      "Epoch: 782     Loss: -1.6747129345435705\n",
      "Epoch: 783     Loss: -1.6749182353461465\n",
      "Epoch: 784     Loss: -1.6751136911842082\n",
      "Epoch: 785     Loss: -1.6753200804646995\n",
      "Epoch: 786     Loss: -1.6755133451338304\n",
      "Epoch: 787     Loss: -1.675728563567985\n",
      "Epoch: 788     Loss: -1.675937912573414\n",
      "Epoch: 789     Loss: -1.6761537401026454\n",
      "Epoch: 790     Loss: -1.676361902037919\n",
      "Epoch: 791     Loss: -1.6765768958129892\n",
      "Epoch: 792     Loss: -1.676783781240896\n",
      "Epoch: 793     Loss: -1.6769976707464749\n",
      "Epoch: 794     Loss: -1.6772042899910848\n",
      "Epoch: 795     Loss: -1.677420470363482\n",
      "Epoch: 796     Loss: -1.6776312169411434\n",
      "Epoch: 797     Loss: -1.6778466467934967\n",
      "Epoch: 798     Loss: -1.6780554966180397\n",
      "Epoch: 799     Loss: -1.6782666422936716\n",
      "Epoch: 800     Loss: -1.6784665958376843\n",
      "Epoch: 801     Loss: -1.6786692327781902\n",
      "Epoch: 802     Loss: -1.6788659420874819\n",
      "Epoch: 803     Loss: -1.6790722278558603\n",
      "Epoch: 804     Loss: -1.679283602270615\n",
      "Epoch: 805     Loss: -1.6794963427236143\n",
      "Epoch: 806     Loss: -1.6797020281390038\n",
      "Epoch: 807     Loss: -1.679909226757509\n",
      "Epoch: 808     Loss: -1.68010003612042\n",
      "Epoch: 809     Loss: -1.6802737701370605\n",
      "Epoch: 810     Loss: -1.6804017971123069\n",
      "Epoch: 811     Loss: -1.6805507851248283\n",
      "Epoch: 812     Loss: -1.6807239367971216\n",
      "Epoch: 813     Loss: -1.6809107975004813\n",
      "Epoch: 814     Loss: -1.6811004738548303\n",
      "Epoch: 815     Loss: -1.6812983386801956\n",
      "Epoch: 816     Loss: -1.6814969559858783\n",
      "Epoch: 817     Loss: -1.681699415266921\n",
      "Epoch: 818     Loss: -1.6818907421887612\n",
      "Epoch: 819     Loss: -1.6820920611239087\n",
      "Epoch: 820     Loss: -1.682280078773983\n",
      "Epoch: 821     Loss: -1.6824824115082637\n",
      "Epoch: 822     Loss: -1.6826802683127235\n",
      "Epoch: 823     Loss: -1.6828826375356254\n",
      "Epoch: 824     Loss: -1.6830729882156859\n",
      "Epoch: 825     Loss: -1.6832727500003608\n",
      "Epoch: 826     Loss: -1.6834634320578716\n",
      "Epoch: 827     Loss: -1.6836612317031443\n",
      "Epoch: 828     Loss: -1.6838503538596974\n",
      "Epoch: 829     Loss: -1.6840475948590068\n",
      "Epoch: 830     Loss: -1.6842383602602684\n",
      "Epoch: 831     Loss: -1.684435456313185\n",
      "Epoch: 832     Loss: -1.684627570222737\n",
      "Epoch: 833     Loss: -1.6848246755573855\n",
      "Epoch: 834     Loss: -1.6850169499038354\n",
      "Epoch: 835     Loss: -1.6852136325676876\n",
      "Epoch: 836     Loss: -1.6854054217127767\n",
      "Epoch: 837     Loss: -1.685597845941121\n",
      "Epoch: 838     Loss: -1.68578531726527\n",
      "Epoch: 839     Loss: -1.685962168251892\n",
      "Epoch: 840     Loss: -1.686149324542567\n",
      "Epoch: 841     Loss: -1.686335249829475\n",
      "Epoch: 842     Loss: -1.6865218151122696\n",
      "Epoch: 843     Loss: -1.686706359053538\n",
      "Epoch: 844     Loss: -1.6868899871027965\n",
      "Epoch: 845     Loss: -1.6870636724241375\n",
      "Epoch: 846     Loss: -1.687247006134312\n",
      "Epoch: 847     Loss: -1.6874253093551204\n",
      "Epoch: 848     Loss: -1.6876103904624424\n",
      "Epoch: 849     Loss: -1.6877935734045058\n",
      "Epoch: 850     Loss: -1.6879831276024195\n",
      "Epoch: 851     Loss: -1.6881718277686917\n",
      "Epoch: 852     Loss: -1.6883644645651914\n",
      "Epoch: 853     Loss: -1.6885564040292333\n",
      "Epoch: 854     Loss: -1.6887515137669487\n",
      "Epoch: 855     Loss: -1.6889443917088203\n",
      "Epoch: 856     Loss: -1.6891380958933009\n",
      "Epoch: 857     Loss: -1.689327126086169\n",
      "Epoch: 858     Loss: -1.6895212033592248\n",
      "Epoch: 859     Loss: -1.6897101860498935\n",
      "Epoch: 860     Loss: -1.6899028711869737\n",
      "Epoch: 861     Loss: -1.6900863080467596\n",
      "Epoch: 862     Loss: -1.6902792038153305\n",
      "Epoch: 863     Loss: -1.6904641730624113\n",
      "Epoch: 864     Loss: -1.6906486608311866\n",
      "Epoch: 865     Loss: -1.6908241445097005\n",
      "Epoch: 866     Loss: -1.6909970657614195\n",
      "Epoch: 867     Loss: -1.6911738432280439\n",
      "Epoch: 868     Loss: -1.691337569589863\n",
      "Epoch: 869     Loss: -1.691504853066919\n",
      "Epoch: 870     Loss: -1.6916744561822663\n",
      "Epoch: 871     Loss: -1.6918556063837373\n",
      "Epoch: 872     Loss: -1.6920314417096765\n",
      "Epoch: 873     Loss: -1.6922157538670122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 874     Loss: -1.692394551575055\n",
      "Epoch: 875     Loss: -1.692578660371835\n",
      "Epoch: 876     Loss: -1.6927577698982466\n",
      "Epoch: 877     Loss: -1.6929415192995059\n",
      "Epoch: 878     Loss: -1.6931215556126964\n",
      "Epoch: 879     Loss: -1.6933051518311966\n",
      "Epoch: 880     Loss: -1.693485208177102\n",
      "Epoch: 881     Loss: -1.6936718492906484\n",
      "Epoch: 882     Loss: -1.6938589434478106\n",
      "Epoch: 883     Loss: -1.6940490114798272\n",
      "Epoch: 884     Loss: -1.6942360762012398\n",
      "Epoch: 885     Loss: -1.694424256898049\n",
      "Epoch: 886     Loss: -1.694603750498513\n",
      "Epoch: 887     Loss: -1.694787884797857\n",
      "Epoch: 888     Loss: -1.6949665156986433\n",
      "Epoch: 889     Loss: -1.6951549709829867\n",
      "Epoch: 890     Loss: -1.6953402186811497\n",
      "Epoch: 891     Loss: -1.6955265669287973\n",
      "Epoch: 892     Loss: -1.6957055972031048\n",
      "Epoch: 893     Loss: -1.695884486888844\n",
      "Epoch: 894     Loss: -1.696048575537938\n",
      "Epoch: 895     Loss: -1.6962225406387401\n",
      "Epoch: 896     Loss: -1.6963854164237915\n",
      "Epoch: 897     Loss: -1.696555391512428\n",
      "Epoch: 898     Loss: -1.6967222538583013\n",
      "Epoch: 899     Loss: -1.6968955831927037\n",
      "Epoch: 900     Loss: -1.6970699985729467\n",
      "Epoch: 901     Loss: -1.6972461816577493\n",
      "Epoch: 902     Loss: -1.697422897612367\n",
      "Epoch: 903     Loss: -1.6975921416066975\n",
      "Epoch: 904     Loss: -1.6977537035537926\n",
      "Epoch: 905     Loss: -1.6979202827657072\n",
      "Epoch: 906     Loss: -1.6980867711438048\n",
      "Epoch: 907     Loss: -1.6982531092157465\n",
      "Epoch: 908     Loss: -1.6984250847421531\n",
      "Epoch: 909     Loss: -1.6985838290096404\n",
      "Epoch: 910     Loss: -1.6987557588879885\n",
      "Epoch: 911     Loss: -1.6989224829051117\n",
      "Epoch: 912     Loss: -1.6990976950005272\n",
      "Epoch: 913     Loss: -1.6992709486712534\n",
      "Epoch: 914     Loss: -1.699451196706522\n",
      "Epoch: 915     Loss: -1.699628614083077\n",
      "Epoch: 916     Loss: -1.6998100730972237\n",
      "Epoch: 917     Loss: -1.6999889455226869\n",
      "Epoch: 918     Loss: -1.7001697270657754\n",
      "Epoch: 919     Loss: -1.700348520313284\n",
      "Epoch: 920     Loss: -1.7005292332523194\n",
      "Epoch: 921     Loss: -1.7007086177809323\n",
      "Epoch: 922     Loss: -1.7008894842616993\n",
      "Epoch: 923     Loss: -1.7010643928521596\n",
      "Epoch: 924     Loss: -1.7012391392967943\n",
      "Epoch: 925     Loss: -1.7014023174855128\n",
      "Epoch: 926     Loss: -1.701564022305737\n",
      "Epoch: 927     Loss: -1.7017217244825424\n",
      "Epoch: 928     Loss: -1.7018909164366123\n",
      "Epoch: 929     Loss: -1.7020565779510515\n",
      "Epoch: 930     Loss: -1.7022255659726855\n",
      "Epoch: 931     Loss: -1.7023895623976346\n",
      "Epoch: 932     Loss: -1.7025600301108748\n",
      "Epoch: 933     Loss: -1.7027296271277998\n",
      "Epoch: 934     Loss: -1.7028983131385258\n",
      "Epoch: 935     Loss: -1.7030636668344792\n",
      "Epoch: 936     Loss: -1.7032261338187185\n",
      "Epoch: 937     Loss: -1.7033849885709673\n",
      "Epoch: 938     Loss: -1.7035476779300178\n",
      "Epoch: 939     Loss: -1.7037150774696002\n",
      "Epoch: 940     Loss: -1.7038810490740735\n",
      "Epoch: 941     Loss: -1.7040511258757058\n",
      "Epoch: 942     Loss: -1.7042187546904322\n",
      "Epoch: 943     Loss: -1.7043839158910832\n",
      "Epoch: 944     Loss: -1.704543693204302\n",
      "Epoch: 945     Loss: -1.7047074198003915\n",
      "Epoch: 946     Loss: -1.7048680639075886\n",
      "Epoch: 947     Loss: -1.7050273545080998\n",
      "Epoch: 948     Loss: -1.7051833831467555\n",
      "Epoch: 949     Loss: -1.7053386969754525\n",
      "Epoch: 950     Loss: -1.7055011690260329\n",
      "Epoch: 951     Loss: -1.705661507534668\n",
      "Epoch: 952     Loss: -1.705825829588624\n",
      "Epoch: 953     Loss: -1.7059902912336737\n",
      "Epoch: 954     Loss: -1.7061556150105353\n",
      "Epoch: 955     Loss: -1.7063229737056174\n",
      "Epoch: 956     Loss: -1.7064901061394402\n",
      "Epoch: 957     Loss: -1.7066588386977053\n",
      "Epoch: 958     Loss: -1.706824456422345\n",
      "Epoch: 959     Loss: -1.7069929867373876\n",
      "Epoch: 960     Loss: -1.70715528421182\n",
      "Epoch: 961     Loss: -1.7073239422709963\n",
      "Epoch: 962     Loss: -1.7074858491799272\n",
      "Epoch: 963     Loss: -1.7076546260908085\n",
      "Epoch: 964     Loss: -1.7078155365829306\n",
      "Epoch: 965     Loss: -1.7079799383355037\n",
      "Epoch: 966     Loss: -1.7081328060771426\n",
      "Epoch: 967     Loss: -1.7082923930962144\n",
      "Epoch: 968     Loss: -1.7084368950544913\n",
      "Epoch: 969     Loss: -1.7085890601460694\n",
      "Epoch: 970     Loss: -1.708732233103399\n",
      "Epoch: 971     Loss: -1.7088907147232808\n",
      "Epoch: 972     Loss: -1.7090497843714156\n",
      "Epoch: 973     Loss: -1.7092168373032914\n",
      "Epoch: 974     Loss: -1.709382342787734\n",
      "Epoch: 975     Loss: -1.7095521420787618\n",
      "Epoch: 976     Loss: -1.709718891656266\n",
      "Epoch: 977     Loss: -1.7098874971122826\n",
      "Epoch: 978     Loss: -1.710049205074014\n",
      "Epoch: 979     Loss: -1.71021532098756\n",
      "Epoch: 980     Loss: -1.7103746240496758\n",
      "Epoch: 981     Loss: -1.710537086624518\n",
      "Epoch: 982     Loss: -1.7106916270727026\n",
      "Epoch: 983     Loss: -1.7108538336858625\n",
      "Epoch: 984     Loss: -1.7110082821241481\n",
      "Epoch: 985     Loss: -1.7111640879035175\n",
      "Epoch: 986     Loss: -1.7113111895545432\n",
      "Epoch: 987     Loss: -1.7114650935160949\n",
      "Epoch: 988     Loss: -1.7116145904725573\n",
      "Epoch: 989     Loss: -1.7117655215200385\n",
      "Epoch: 990     Loss: -1.711918788305585\n",
      "Epoch: 991     Loss: -1.7120736872558333\n",
      "Epoch: 992     Loss: -1.7122335554587476\n",
      "Epoch: 993     Loss: -1.7123906977853978\n",
      "Epoch: 994     Loss: -1.7125524507084349\n",
      "Epoch: 995     Loss: -1.712709331088453\n",
      "Epoch: 996     Loss: -1.7128707897997781\n",
      "Epoch: 997     Loss: -1.7130250072314746\n",
      "Epoch: 998     Loss: -1.7131843166352736\n",
      "Epoch: 999     Loss: -1.713336444069803\n",
      "Epoch: 1000     Loss: -1.7134953531520534\n",
      "Epoch: 1001     Loss: -1.7136432546052727\n",
      "Epoch: 1002     Loss: -1.7138033972553157\n",
      "Epoch: 1003     Loss: -1.7139554251643716\n",
      "Epoch: 1004     Loss: -1.7141181701800725\n",
      "Epoch: 1005     Loss: -1.714273424772312\n",
      "Epoch: 1006     Loss: -1.7144374668730407\n",
      "Epoch: 1007     Loss: -1.7145951411849536\n",
      "Epoch: 1008     Loss: -1.7147599494414447\n",
      "Epoch: 1009     Loss: -1.714919209432889\n",
      "Epoch: 1010     Loss: -1.7150840726823386\n",
      "Epoch: 1011     Loss: -1.7152440866745342\n",
      "Epoch: 1012     Loss: -1.7154084603241961\n",
      "Epoch: 1013     Loss: -1.7155661598599694\n",
      "Epoch: 1014     Loss: -1.7157231807341622\n",
      "Epoch: 1015     Loss: -1.7158738092181707\n",
      "Epoch: 1016     Loss: -1.716025951367877\n",
      "Epoch: 1017     Loss: -1.7161806326538875\n",
      "Epoch: 1018     Loss: -1.7163390587596123\n",
      "Epoch: 1019     Loss: -1.7164933642542854\n",
      "Epoch: 1020     Loss: -1.716651325693293\n",
      "Epoch: 1021     Loss: -1.7168062726906381\n",
      "Epoch: 1022     Loss: -1.7169612791370892\n",
      "Epoch: 1023     Loss: -1.7171155865909344\n",
      "Epoch: 1024     Loss: -1.717269854232268\n",
      "Epoch: 1025     Loss: -1.7174235950800543\n",
      "Epoch: 1026     Loss: -1.7175744556084067\n",
      "Epoch: 1027     Loss: -1.717726559530955\n",
      "Epoch: 1028     Loss: -1.7178777843744284\n",
      "Epoch: 1029     Loss: -1.7180283161215604\n",
      "Epoch: 1030     Loss: -1.7181776510262763\n",
      "Epoch: 1031     Loss: -1.7183251491408222\n",
      "Epoch: 1032     Loss: -1.7184681208563741\n",
      "Epoch: 1033     Loss: -1.7186054133200261\n",
      "Epoch: 1034     Loss: -1.7187421166380163\n",
      "Epoch: 1035     Loss: -1.7188799298185236\n",
      "Epoch: 1036     Loss: -1.7190240779598636\n",
      "Epoch: 1037     Loss: -1.7191688561371075\n",
      "Epoch: 1038     Loss: -1.7193190369123665\n",
      "Epoch: 1039     Loss: -1.7194663116142943\n",
      "Epoch: 1040     Loss: -1.719614805664207\n",
      "Epoch: 1041     Loss: -1.719754745793466\n",
      "Epoch: 1042     Loss: -1.7198843183749815\n",
      "Epoch: 1043     Loss: -1.720022595214097\n",
      "Epoch: 1044     Loss: -1.720167288368306\n",
      "Epoch: 1045     Loss: -1.7203097610672122\n",
      "Epoch: 1046     Loss: -1.720452393341021\n",
      "Epoch: 1047     Loss: -1.7206037987564722\n",
      "Epoch: 1048     Loss: -1.7207565455027378\n",
      "Epoch: 1049     Loss: -1.7209109601101784\n",
      "Epoch: 1050     Loss: -1.7210632234171932\n",
      "Epoch: 1051     Loss: -1.7212172151970786\n",
      "Epoch: 1052     Loss: -1.7213682256956262\n",
      "Epoch: 1053     Loss: -1.721523514049892\n",
      "Epoch: 1054     Loss: -1.7216753385062593\n",
      "Epoch: 1055     Loss: -1.721826786668499\n",
      "Epoch: 1056     Loss: -1.721973571205981\n",
      "Epoch: 1057     Loss: -1.722121966439046\n",
      "Epoch: 1058     Loss: -1.7222671857700724\n",
      "Epoch: 1059     Loss: -1.7224169886128307\n",
      "Epoch: 1060     Loss: -1.722565778914569\n",
      "Epoch: 1061     Loss: -1.7227176031983287\n",
      "Epoch: 1062     Loss: -1.7228656830533486\n",
      "Epoch: 1063     Loss: -1.7230138194313247\n",
      "Epoch: 1064     Loss: -1.7231540335481637\n",
      "Epoch: 1065     Loss: -1.723299480518473\n",
      "Epoch: 1066     Loss: -1.7234402364135668\n",
      "Epoch: 1067     Loss: -1.723584276819462\n",
      "Epoch: 1068     Loss: -1.7237195272597587\n",
      "Epoch: 1069     Loss: -1.723861008276027\n",
      "Epoch: 1070     Loss: -1.7240028692686928\n",
      "Epoch: 1071     Loss: -1.7241489175588889\n",
      "Epoch: 1072     Loss: -1.724293421025109\n",
      "Epoch: 1073     Loss: -1.7244424900473883\n",
      "Epoch: 1074     Loss: -1.7245908701263641\n",
      "Epoch: 1075     Loss: -1.7247414680149415\n",
      "Epoch: 1076     Loss: -1.7248897374600318\n",
      "Epoch: 1077     Loss: -1.7250398727764535\n",
      "Epoch: 1078     Loss: -1.7251864093478964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1079     Loss: -1.7253348274206322\n",
      "Epoch: 1080     Loss: -1.725478281227124\n",
      "Epoch: 1081     Loss: -1.7256241111515769\n",
      "Epoch: 1082     Loss: -1.725766317480244\n",
      "Epoch: 1083     Loss: -1.7259127851499705\n",
      "Epoch: 1084     Loss: -1.7260586607721848\n",
      "Epoch: 1085     Loss: -1.726208169534482\n",
      "Epoch: 1086     Loss: -1.7263535056647281\n",
      "Epoch: 1087     Loss: -1.7264991385621804\n",
      "Epoch: 1088     Loss: -1.7266438512698472\n",
      "Epoch: 1089     Loss: -1.726790081770068\n",
      "Epoch: 1090     Loss: -1.7269322325385181\n",
      "Epoch: 1091     Loss: -1.727076837445426\n",
      "Epoch: 1092     Loss: -1.7272195806846813\n",
      "Epoch: 1093     Loss: -1.7273629340485415\n",
      "Epoch: 1094     Loss: -1.7275059098350882\n",
      "Epoch: 1095     Loss: -1.7276481761332252\n",
      "Epoch: 1096     Loss: -1.7277877185456845\n",
      "Epoch: 1097     Loss: -1.7279210130060727\n",
      "Epoch: 1098     Loss: -1.7280548953611992\n",
      "Epoch: 1099     Loss: -1.72818411579971\n",
      "Epoch: 1100     Loss: -1.728318914034983\n",
      "Epoch: 1101     Loss: -1.7284497823973242\n",
      "Epoch: 1102     Loss: -1.7285862723197776\n",
      "Epoch: 1103     Loss: -1.7287186524346763\n",
      "Epoch: 1104     Loss: -1.7288532163672878\n",
      "Epoch: 1105     Loss: -1.7289761605099931\n",
      "Epoch: 1106     Loss: -1.7290960621610318\n",
      "Epoch: 1107     Loss: -1.7292341190392841\n",
      "Epoch: 1108     Loss: -1.7293721990590778\n",
      "Epoch: 1109     Loss: -1.7295102211204474\n",
      "Epoch: 1110     Loss: -1.7296485703189814\n",
      "Epoch: 1111     Loss: -1.7297884854891705\n",
      "Epoch: 1112     Loss: -1.729927982952433\n",
      "Epoch: 1113     Loss: -1.730070367112802\n",
      "Epoch: 1114     Loss: -1.7302063153930998\n",
      "Epoch: 1115     Loss: -1.7303514810544542\n",
      "Epoch: 1116     Loss: -1.730492300481257\n",
      "Epoch: 1117     Loss: -1.7306382551559292\n",
      "Epoch: 1118     Loss: -1.7307821078222347\n",
      "Epoch: 1119     Loss: -1.730927455615336\n",
      "Epoch: 1120     Loss: -1.731067910467812\n",
      "Epoch: 1121     Loss: -1.7312100104058419\n",
      "Epoch: 1122     Loss: -1.731346817960423\n",
      "Epoch: 1123     Loss: -1.7314842687055003\n",
      "Epoch: 1124     Loss: -1.731614665231469\n",
      "Epoch: 1125     Loss: -1.7317432738150524\n",
      "Epoch: 1126     Loss: -1.7318748189421669\n",
      "Epoch: 1127     Loss: -1.7320110003946283\n",
      "Epoch: 1128     Loss: -1.732151258394691\n",
      "Epoch: 1129     Loss: -1.7322932291044961\n",
      "Epoch: 1130     Loss: -1.7324367699062728\n",
      "Epoch: 1131     Loss: -1.732579876316631\n",
      "Epoch: 1132     Loss: -1.7327226784823533\n",
      "Epoch: 1133     Loss: -1.7328639864468744\n",
      "Epoch: 1134     Loss: -1.7330047350040643\n",
      "Epoch: 1135     Loss: -1.7331438743588017\n",
      "Epoch: 1136     Loss: -1.7332819520862766\n",
      "Epoch: 1137     Loss: -1.733415704281887\n",
      "Epoch: 1138     Loss: -1.733548935916765\n",
      "Epoch: 1139     Loss: -1.733681928541995\n",
      "Epoch: 1140     Loss: -1.73381684254727\n",
      "Epoch: 1141     Loss: -1.7339466453558157\n",
      "Epoch: 1142     Loss: -1.734079953271241\n",
      "Epoch: 1143     Loss: -1.7342143056239305\n",
      "Epoch: 1144     Loss: -1.7343488168806374\n",
      "Epoch: 1145     Loss: -1.7344821862941624\n",
      "Epoch: 1146     Loss: -1.7346150999820982\n",
      "Epoch: 1147     Loss: -1.7347497141466766\n",
      "Epoch: 1148     Loss: -1.7348796397819053\n",
      "Epoch: 1149     Loss: -1.73501208676948\n",
      "Epoch: 1150     Loss: -1.735134526565817\n",
      "Epoch: 1151     Loss: -1.7352632433998256\n",
      "Epoch: 1152     Loss: -1.7353815381211732\n",
      "Epoch: 1153     Loss: -1.7355136614926239\n",
      "Epoch: 1154     Loss: -1.7356452541927647\n",
      "Epoch: 1155     Loss: -1.7357819608734708\n",
      "Epoch: 1156     Loss: -1.735915610221388\n",
      "Epoch: 1157     Loss: -1.7360498073468869\n",
      "Epoch: 1158     Loss: -1.7361831075226413\n",
      "Epoch: 1159     Loss: -1.7363171750665847\n",
      "Epoch: 1160     Loss: -1.7364450180698248\n",
      "Epoch: 1161     Loss: -1.736574708548336\n",
      "Epoch: 1162     Loss: -1.7367051367532405\n",
      "Epoch: 1163     Loss: -1.736838256946464\n",
      "Epoch: 1164     Loss: -1.7369690676518106\n",
      "Epoch: 1165     Loss: -1.7371019527724283\n",
      "Epoch: 1166     Loss: -1.7372345015372228\n",
      "Epoch: 1167     Loss: -1.7373689731732416\n",
      "Epoch: 1168     Loss: -1.7375036282754164\n",
      "Epoch: 1169     Loss: -1.7376397928350393\n",
      "Epoch: 1170     Loss: -1.7377759846998944\n",
      "Epoch: 1171     Loss: -1.7379127878539429\n",
      "Epoch: 1172     Loss: -1.7380469565698577\n",
      "Epoch: 1173     Loss: -1.738179947260075\n",
      "Epoch: 1174     Loss: -1.7383063603120026\n",
      "Epoch: 1175     Loss: -1.7384321475322624\n",
      "Epoch: 1176     Loss: -1.7385521755889113\n",
      "Epoch: 1177     Loss: -1.7386790098888998\n",
      "Epoch: 1178     Loss: -1.7388030443402103\n",
      "Epoch: 1179     Loss: -1.7389306694964528\n",
      "Epoch: 1180     Loss: -1.7390569706460237\n",
      "Epoch: 1181     Loss: -1.7391837338724803\n",
      "Epoch: 1182     Loss: -1.739310235854883\n",
      "Epoch: 1183     Loss: -1.7394364765842063\n",
      "Epoch: 1184     Loss: -1.7395581828584727\n",
      "Epoch: 1185     Loss: -1.7396821041336197\n",
      "Epoch: 1186     Loss: -1.7398036336682503\n",
      "Epoch: 1187     Loss: -1.7399310674031898\n",
      "Epoch: 1188     Loss: -1.7400572388133484\n",
      "Epoch: 1189     Loss: -1.7401870230797079\n",
      "Epoch: 1190     Loss: -1.740317819280417\n",
      "Epoch: 1191     Loss: -1.7404500887077468\n",
      "Epoch: 1192     Loss: -1.7405821472673304\n",
      "Epoch: 1193     Loss: -1.7407153801427866\n",
      "Epoch: 1194     Loss: -1.7408487198786742\n",
      "Epoch: 1195     Loss: -1.7409809279776591\n",
      "Epoch: 1196     Loss: -1.7411122974287025\n",
      "Epoch: 1197     Loss: -1.7412423263273915\n",
      "Epoch: 1198     Loss: -1.74137476665792\n",
      "Epoch: 1199     Loss: -1.7415051862942776\n",
      "Epoch: 1200     Loss: -1.7416356961357276\n",
      "Epoch: 1201     Loss: -1.7417619548415426\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # 输出的维度\n",
    "    outdim_size = 2\n",
    "\n",
    "    # 两个view的输入\n",
    "    input_shape1 = 2\n",
    "    input_shape2 = 10\n",
    "\n",
    "    # 网络架构设置\n",
    "    layer_sizes1 = [20, 40, 20, outdim_size]\n",
    "    layer_sizes2 = [20, 40, 20, outdim_size]\n",
    "\n",
    "    # 超参数设置\n",
    "    learning_rate = 1e-3\n",
    "    epoch_num = 2000\n",
    "    reg_par = 1e-5\n",
    "    use_all_singular_values = False\n",
    "\n",
    "    # 搭建网络\n",
    "    model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
    "                    input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
    "    solver = Solver(model, outdim_size, epoch_num, learning_rate, reg_par, device=device)\n",
    "    \n",
    "    # 训练\n",
    "    train1, train2 = X1, X2\n",
    "    solver.fit(train1, train2)\n",
    "\n",
    "    # 测试\n",
    "    loss, outputs = solver.test(train1, train2)\n",
    "    \n",
    "    print(\"\\n**********单独在原始数据集上训练LR和SVM分类器的精度和AUC*************\")\n",
    "    print(\"View 1(LR) ACC : \", LR_2(X1, y)[0] * 100, \"%       AUC:\", LR_2(X1, y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(X2, y)[0] * 100, \"%       AUC:\", LR_2(X2, y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(X1, y)[0] * 100, \"%       AUC:\", svm_2(X1, y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(X2, y)[0] * 100, \"%       AUC:\", svm_2(X2, y)[1], \"\\n\")\n",
    "    \n",
    "    print(\"**********在抽取出的共同空间上训练LR和SVM分类器的精度和AUC*************\")\n",
    "    print(\"View 1(LR) ACC : \", LR_2(outputs[0], y)[0] * 100, \"%       AUC:\", LR_2(outputs[0], y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(outputs[1], y)[0] * 100, \"%       AUC:\", LR_2(outputs[1], y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(outputs[0], y)[0] * 100, \"%       AUC:\", svm_2(outputs[0], y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(outputs[1], y)[0] * 100, \"%       AUC:\", svm_2(outputs[1], y)[1])\n",
    "\n",
    "    #可视化映射到隐空间中的状态，尾数1代表View 1\n",
    "    visualization_LR(outputs[0], y, 'View 1')\n",
    "    visualization_LR(outputs[1], y, 'View 2')\n",
    "    visualization_SVM(outputs[0], y, 'View 1')\n",
    "    visualization_SVM(outputs[1], y, 'View 2')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "data = pd.read_csv(\"UCI_Credit_Card.csv\")\n",
    "data.isnull().sum()\n",
    "y = data['default.payment.next.month']\n",
    "x_ori = data.drop(['ID', 'default.payment.next.month'], axis=1)\n",
    "x_ori = x_ori.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\n",
    "x_ori.shape\n",
    "\n",
    "train_a1 = np.array(x_ori.iloc[:, 0:11])\n",
    "train_b1 = np.array(x_ori.iloc[:, 11:23])\n",
    "\n",
    "\n",
    "train_a = torch.from_numpy(train_a1).double()\n",
    "train_b = torch.from_numpy(train_b1).double()\n",
    "train_a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.from_numpy(train_a1).double()\n",
    "X2 = torch.from_numpy(train_b1).double()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # 输出的维度\n",
    "    outdim_size = 2\n",
    "\n",
    "    # 两个view的输入\n",
    "    input_shape1 = 11\n",
    "    input_shape2 = 12\n",
    "\n",
    "    # 网络架构设置\n",
    "    layer_sizes1 = [25, 50, 25, outdim_size]\n",
    "    layer_sizes2 = [25, 50, 25, outdim_size]\n",
    "\n",
    "    # 超参数设置\n",
    "    learning_rate = 1e-3\n",
    "    epoch_num = 20000\n",
    "    reg_par = 1e-5\n",
    "    use_all_singular_values = False\n",
    "\n",
    "    # 搭建网络\n",
    "    model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
    "                    input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
    "    solver = Solver(model, outdim_size, epoch_num, learning_rate, reg_par, device=device)\n",
    "    \n",
    "    # 训练\n",
    "    train1, train2 = X1, X2\n",
    "    solver.fit(train1, train2)\n",
    "\n",
    "    # 测试\n",
    "    loss, outputs = solver.test(train1, train2)\n",
    "    \n",
    "    print(\"\\n**********单独在原始数据集上训练LR和SVM分类器的精度和AUC*************\")\n",
    "    print(\"View 1(LR) ACC : \", LR_2(X1, y)[0] * 100, \"%       AUC:\", LR_2(X1, y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(X2, y)[0] * 100, \"%       AUC:\", LR_2(X2, y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(X1, y)[0] * 100, \"%       AUC:\", svm_2(X1, y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(X2, y)[0] * 100, \"%       AUC:\", svm_2(X2, y)[1], \"\\n\")\n",
    "    \n",
    "    print(\"**********在抽取出的共同空间上训练LR和SVM分类器的精度和AUC*************\")\n",
    "    print(\"View 1(LR) ACC : \", LR_2(outputs[0], y)[0] * 100, \"%       AUC:\", LR_2(outputs[0], y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(outputs[1], y)[0] * 100, \"%       AUC:\", LR_2(outputs[1], y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(outputs[0], y)[0] * 100, \"%       AUC:\", svm_2(outputs[0], y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(outputs[1], y)[0] * 100, \"%       AUC:\", svm_2(outputs[1], y)[1])\n",
    "    \n",
    "    #可视化映射到隐空间中的状态，尾数1代表View 1\n",
    "    visualization_LR(outputs[0], y, 'View 1')\n",
    "    visualization_LR(outputs[1], y, 'View 2')\n",
    "    visualization_SVM(outputs[0], y, 'View 1')\n",
    "    visualization_SVM(outputs[1], y, 'View 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Give Me Some Credit\n",
    "df = pd.read_csv(\"cs-training.csv\")\n",
    "df = df.drop(\"Unnamed: 0\", axis=1) # drop id column\n",
    "df = df.loc[df[\"DebtRatio\"] <= df[\"DebtRatio\"].quantile(0.975)]\n",
    "df = df.loc[(df[\"RevolvingUtilizationOfUnsecuredLines\"] >= 0) & (df[\"RevolvingUtilizationOfUnsecuredLines\"] < 13)]\n",
    "df = df.loc[df[\"NumberOfTimes90DaysLate\"] <= 17]\n",
    "dependents_mode = df[\"NumberOfDependents\"].mode()[0]\n",
    "df[\"NumberOfDependents\"] = df[\"NumberOfDependents\"].fillna(dependents_mode)\n",
    "income_median = df[\"MonthlyIncome\"].median()\n",
    "df[\"MonthlyIncome\"] = df[\"MonthlyIncome\"].fillna(income_median)\n",
    "X = df.drop(\"SeriousDlqin2yrs\", axis=1)\n",
    "Y = df[\"SeriousDlqin2yrs\"]\n",
    "train_a1 = np.array(X.iloc[:, 0:5])\n",
    "train_b1 = np.array(X.iloc[:, 5:10])\n",
    "\n",
    "y = numpy.array(Y)\n",
    "train_a1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.from_numpy(train_a1).double()\n",
    "X2 = torch.from_numpy(train_b1).double()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # 输出的维度\n",
    "    outdim_size = 2\n",
    "\n",
    "    # 两个view的输入\n",
    "    input_shape1 = 5\n",
    "    input_shape2 = 5\n",
    "\n",
    "    # 网络架构设置\n",
    "    layer_sizes1 = [20, 40, 20, outdim_size]\n",
    "    layer_sizes2 = [20, 40, 20, outdim_size]\n",
    "\n",
    "    # 超参数设置\n",
    "    learning_rate = 1e-3\n",
    "    epoch_num = 1500\n",
    "    reg_par = 1e-5\n",
    "    use_all_singular_values = False\n",
    "\n",
    "    # 搭建网络\n",
    "    model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
    "                    input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
    "    solver = Solver(model, outdim_size, epoch_num, learning_rate, reg_par, device=device)\n",
    "    \n",
    "    # 训练\n",
    "    train1, train2 = X1, X2\n",
    "    solver.fit(train1, train2)\n",
    "\n",
    "    # 测试\n",
    "    loss, outputs = solver.test(train1, train2)\n",
    "    \n",
    "    print(\"**********单独在原始数据集上训练LR和SVM分类器的精度和AUC*************\") \n",
    "    print(\"View 1(LR) ACC : \", LR_2(X1, y)[0] * 100, \"%       AUC:\", LR_2(X1, y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(X2, y)[0] * 100, \"%       AUC:\", LR_2(X2, y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(X1, y)[0] * 100, \"%       AUC:\", svm_2(X1, y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(X2, y)[0] * 100, \"%       AUC:\", svm_2(X2, y)[1])\n",
    "    \n",
    "    print(\"**********在抽取出的共同空间上训练LR和SVM分类器的精度和AUC*************\")\n",
    "    print(\"View 1(LR) ACC : \", LR_2(outputs[0], y)[0] * 100, \"%       AUC:\", LR_2(outputs[0], y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(outputs[1], y)[0] * 100, \"%       AUC:\", LR_2(outputs[1], y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(outputs[0], y)[0] * 100, \"%       AUC:\", svm_2(outputs[0], y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(outputs[1], y)[0] * 100, \"%       AUC:\", svm_2(outputs[1], y)[1])\n",
    "    \n",
    "    #可视化映射到隐空间中的状态，尾数1代表View 1\n",
    "    visualization_LR(outputs[0], y, 'View 1')\n",
    "    visualization_LR(outputs[1], y, 'View 2')\n",
    "    visualization_SVM(outputs[0], y, 'View 1')\n",
    "    visualization_SVM(outputs[1], y, 'View 2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "  \n",
    "def load_mnist_train():\n",
    "  labels_path = \"train-labels.idx1-ubyte\"\n",
    "   \n",
    "  images_path = \"train-images.idx3-ubyte\"\n",
    "   \n",
    "  with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    labels = np.fromfile(lbpath,dtype=np.uint8) \n",
    "  \n",
    "  with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n",
    "  return images, labels\n",
    "\n",
    "def load_mnist_test():\n",
    "  labels_path = \"t10k-labels.idx1-ubyte\"\n",
    "   \n",
    "  images_path = \"t10k-images.idx3-ubyte\"\n",
    "   \n",
    "  with open(labels_path, 'rb') as lbpath:\n",
    "    magic, n = struct.unpack('>II',lbpath.read(8))\n",
    "    labels = np.fromfile(lbpath,dtype=np.uint8) \n",
    "  \n",
    "  with open(images_path, 'rb') as imgpath:\n",
    "    magic, num, rows, cols = struct.unpack('>IIII',imgpath.read(16))\n",
    "    images = np.fromfile(imgpath,dtype=np.uint8).reshape(len(labels), 784)\n",
    "  return images, labels\n",
    "\n",
    "train_temp = load_mnist_train()[0]\n",
    "y = load_mnist_train()[1] % 2\n",
    "\n",
    "train_a1 = np.array(train_temp[:, 0:392])\n",
    "train_b1 = np.array(train_temp[:, 392:784])\n",
    "\n",
    "train_a1.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = torch.from_numpy(train_a1).double()\n",
    "X2 = torch.from_numpy(train_b1).double()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "    # 输出的维度\n",
    "    outdim_size = 2\n",
    "\n",
    "    # 两个view的输入\n",
    "    input_shape1 = 392\n",
    "    input_shape2 = 392\n",
    "\n",
    "    # 网络架构设置\n",
    "    layer_sizes1 = [512, 1024, 512, outdim_size]\n",
    "    layer_sizes2 = [512, 1024, 512, outdim_size]\n",
    "\n",
    "    # 超参数设置\n",
    "    learning_rate = 1e-3\n",
    "    epoch_num = 2000\n",
    "    reg_par = 1e-5\n",
    "    use_all_singular_values = False\n",
    "\n",
    "    # 搭建网络\n",
    "    model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
    "                    input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
    "    solver = Solver(model, outdim_size, epoch_num, learning_rate, reg_par, device=device)\n",
    "    \n",
    "    # 训练\n",
    "    train1, train2 = X1, X2\n",
    "    solver.fit(train1, train2)\n",
    "\n",
    "    # 测试\n",
    "    loss, outputs = solver.test(train1, train2)\n",
    "  \n",
    "    print(\"**********单独在原始数据集上训练LR和SVM分类器的精度和AUC*************\")   \n",
    "    print(\"View 1(LR) ACC : \", LR_2(X1, y)[0] * 100, \"%       AUC:\", LR_2(X1, y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(X2, y)[0] * 100, \"%       AUC:\", LR_2(X2, y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(X1, y)[0] * 100, \"%       AUC:\", svm_2(X1, y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(X2, y)[0] * 100, \"%       AUC:\", svm_2(X2, y)[1])\n",
    "    \n",
    "    print(\"**********在抽取出的共同空间上训练LR和SVM分类器的精度和AUC*************\") \n",
    "    print(\"View 1(LR) ACC : \", LR_2(outputs[0], y)[0] * 100, \"%       AUC:\", LR_2(outputs[0], y)[1])\n",
    "    print(\"View 2(LR) ACC : \", LR_2(outputs[1], y)[0] * 100, \"%       AUC:\", LR_2(outputs[1], y)[1])\n",
    "    print(\"View 1(SVM) ACC : \", svm_2(outputs[0], y)[0] * 100, \"%       AUC:\", svm_2(outputs[0], y)[1])\n",
    "    print(\"View 2(SVM) ACC : \", svm_2(outputs[1], y)[0] * 100, \"%       AUC:\", svm_2(outputs[1], y)[1])\n",
    "    \n",
    "    #可视化映射到隐空间中的状态，尾数1代表View 1\n",
    "    visualization_LR(outputs[0], y, 'View 1')\n",
    "    visualization_LR(outputs[1], y, 'View 2')\n",
    "    visualization_SVM(outputs[0], y, 'View 1')\n",
    "    visualization_SVM(outputs[1], y, 'View 2')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Pytorch]",
   "language": "python",
   "name": "conda-env-Pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
